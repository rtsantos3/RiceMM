{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03a656bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T11:08:41.205406Z",
     "start_time": "2023-03-23T11:08:39.074105Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "import cobra\n",
    "import libsbml\n",
    "import pandas as pd\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import memote\n",
    "import csv\n",
    "import pytest\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xlsxwriter\n",
    "import path \n",
    "import datetime\n",
    "import scipy.sparse as sp\n",
    "import warnings\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from functools import partial  \n",
    "from parallelbar import progress_map\n",
    "\n",
    "\n",
    "from cobra import sampling\n",
    "from cobra import Reaction\n",
    "\n",
    "#Change working dir first, ty ChatGPT, much loves\n",
    "cwd = os.getcwd()\n",
    "# Split the path into a list of directories\n",
    "directories = cwd.split(os.sep)\n",
    "# Remove the last two directories from the list\n",
    "directories = directories[:-2]\n",
    "# Join the directories back into a path\n",
    "new_cwd = os.sep.join(directories)\n",
    "# Change the current working directory to the new path\n",
    "os.chdir(new_cwd)\n",
    "\n",
    "sys.path.append(\"./src\")\n",
    "\n",
    "import model_manipulation  as mm\n",
    "\n",
    "#Add a log object to scripts\n",
    "logs_dir ='./logs/'\n",
    "\n",
    "\n",
    "\n",
    "# Function to get the current date and time in a specific format\n",
    "def get_current_datetime():\n",
    "    return datetime.datetime.now().strftime(\"[%Y%m%d-%H:%M:%S] \")\n",
    "\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, file):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(file, \"a\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "        self.log.flush()  # Ensure the log is immediately written to the file\n",
    "\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19cbcb9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T11:14:41.725034Z",
     "start_time": "2023-03-23T11:11:53.801Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This codeblock is to define some of the functions used for modelling\n",
    "\n",
    "##UPDATED JULY 28 2023\n",
    "\n",
    "##NOTES:\n",
    "##Fixed wrong encoding of reactions for Malate dehydrogenase as well as NADP ME.\n",
    "\n",
    "\n",
    "\n",
    "#Define linear relationship between PPFD and Cellular maintainance costs\n",
    "#This formula comes from Topfer et al (2020) where she defined NGAM in a linear relationship with incident light\n",
    "inf=1e6\n",
    "\n",
    "def generate_constraint(model,reaction, name, lb, ub):\n",
    "    reaction_fex = model.reactions.get_by_id(name).flux_expression\n",
    "    constraint = model.problem.Constraint(reaction_fex, lb=lb, ub=ub)\n",
    "    constraint.name = name + '_constraint'\n",
    "    model.add_cons_vars\n",
    "\n",
    "def compute_ngam_atp(ppfd):\n",
    "    v_atp = 0.0049*ppfd + 2.7851\n",
    "    return v_atp\n",
    "\n",
    "\n",
    "#This function is used to set the inputs to the model used. \n",
    "def define_model_medium(model, co2, o2, ppfd, \n",
    "                        medium_dir='./misc/photo_medium.csv', no3=inf, h2o=inf, h=inf, \n",
    "                        nh4=inf, pi=inf):\n",
    "    model_photo_media = mm.read_medium_csv(medium_dir, model)\n",
    "    model_photo_media['EX_no3(e)'] = no3\n",
    "    model_photo_media['EX_h2o(e)'] = h2o\n",
    "    model_photo_media['EX_h(e)'] = h\n",
    "    model_photo_media['EX_nh4(e)'] = nh4\n",
    "    model_photo_media['EX_co2(e)'] = co2\n",
    "    model_photo_media['EX_o2(e)'] = o2\n",
    "    model_photo_media['EX_photonVis(e)'] = ppfd\n",
    "    model_photo_media['EX_pi(e)'] = pi\n",
    "    #Set set model medium as model\n",
    "#     print('Added model medium')\n",
    "    return model_photo_media\n",
    "\n",
    "    \n",
    "def turn_off_cofac_cycles(model, inact_dir='./misc/leaf_inactivated.tsv'):\n",
    "    file = csv.reader(open(inact_dir), delimiter='\\t')\n",
    "    leaf_inactive_rxns = list()\n",
    "    for rows in file:\n",
    "        row_m = str()\n",
    "        row_bs = str()\n",
    "        for rxns in rows:\n",
    "            row_m += str(rxns) + \"_M\"\n",
    "            row_bs += str(rxns) + \"_BS\"\n",
    "        leaf_inactive_rxns.append(row_m)\n",
    "        leaf_inactive_rxns.append(row_bs)\n",
    "        \n",
    "    for rxns in model.reactions:\n",
    "        if rxns.id in leaf_inactive_rxns:\n",
    "            rxns.bounds = (0,0)\n",
    "#     print('Successfully turned off cofactor-cycling reactions')\n",
    "\n",
    "    \n",
    "# #Add constraints to model\n",
    "#This code block contains constraints that would simulate the assimilation rates of bs and m cells in a two-cell system (such as those seen near the midvein region of rice leaves)\n",
    "# #BS photon flux must be the same/less than M flux (Adapted from B&B, 2019)\n",
    "# photon_import = model.reactions.get_by_id(\"EX_photonVis(e)\")\n",
    "def add_tissue_constraints(model):\n",
    "    #For input fluxes for light, we will set the flux ratio to 10:1 to reflect the anatomical proportions of our model ()\n",
    "    \n",
    "    BS_photon_import = model.reactions.PRISM_white_LED_BS\n",
    "    M_photon_import = model.reactions.PRISM_white_LED_M\n",
    "\n",
    "    #Set photon flux ratio to 10:1\n",
    "    photon_flux = mm.set_fix_flux_ratio({M_photon_import.id:10, BS_photon_import.id:1},model)\n",
    "    model.add_cons_vars(photon_flux)\n",
    "\n",
    "    \n",
    "    #UPDATE: Change CO2 intake to the M Cell instead rather than set a ratio, which is a better assumption overall. Assume na lang that external gasses are assimilated\n",
    "    #Via the M cell.\n",
    "    #From Morrison et al 2005 -- Lateral diffusion of Gases is unlikely to support photosynthesis due to the\n",
    "    #assimilation of diffused CO2 in tissues prior to BS//\n",
    "    model.reactions.CO2tex_BS.bounds = (0,0)\n",
    "    model.reactions.O2tex_BS.bounds = (0,0)\n",
    "    \n",
    "    #UPDATE: This assumption does not hold considering that recent transcriptomic analysis confirms that \n",
    "    #the bundle sheath is involved in the assimilation of inorganic nutrients, including nitrogen (nitrates/ammonia), and \n",
    "    #Sulfates. In turn, this will be implemented by simply setting the exchanges to the M cell to 0. (Hua et al, 2021)\n",
    "    model.reactions.SO3tex_M.bounds = (0,0)\n",
    "    model.reactions.SO4tex_M.bounds = (0,0)\n",
    "    model.reactions.NH4tex_M.bounds = (0,0)\n",
    "    model.reactions.NO3tex_M.bounds = (0,0)\n",
    "    \n",
    "    #Model will also constraint H2O input to BS cell only as it is also assumed that BS tissue in rice is specialized for H2O transport (Hua et al. 2021)\n",
    "    #There is a demand reaction naman for H2O for the M cell which is not connected to the BS H2Otex\n",
    "    #Restrict H2O transport to be unidirectional from the BS cell\n",
    "    model.reactions.H2Otex_M.bounds = (0, 0)\n",
    "    model.reactions.h2o_pd.bounds = (-inf, 0)\n",
    "    \n",
    "    #need to turn off HCO import as the model incorrectly transfers fixed HCO to the BS cell via the common pool compartment\n",
    "    model.reactions.HCO3tex_M.bounds = (0,0)\n",
    "    model.reactions.HCO3tex_BS.bounds = (0,0)\n",
    "    \n",
    "    #Turn off extracellular Glycine transport \n",
    "    model.reactions.GLYtex_M.bounds = (0,0)\n",
    "    model.reactions.GLYtex_BS.bounds = (0,0)\n",
    "    \n",
    "    #Turn off other Demand reactions that may serve as sinks for the model except DM_Phloem_BS (Which represents the output of photoassimilate thru the BS cell\n",
    "    model.reactions.DM_Phloem_M.bounds = (0,0)\n",
    "    model.reactions.Straw_Biomass_M.bounds = (0,0)\n",
    "    model.reactions.Straw_Biomass_BS.bounds = (0,0)\n",
    "    model.reactions.Coleoptile_Biomass_M.bounds = (0,0)\n",
    "    model.reactions.Coleoptile_Biomass_BS.bounds = (0,0)\n",
    "    model.reactions.DM_Phloem_BS.bounds = (0, inf)\n",
    "    \n",
    "\n",
    "def add_enzyme_constraints(model, \n",
    "                           wt_pepc = 0, \n",
    "                           wt_mdh = 11.18, \n",
    "                           wt_nadp_me = 0.14, \n",
    "                           wt_ppdk=0.31,\n",
    "                          wt_CA=7.5):\n",
    "    \n",
    "    \n",
    "    # #This code block contains constraints specific for enzyme rate constraints\n",
    "    #This approach is derived from Bogart & Myers (2016) where they constrained the enzyme rate \n",
    "    #fluxes in each of the 2-cell segments to a specific upper bound while keeping the lower bound\n",
    "    #At 0. For reversible reactions the lower bounds are set to the same value\n",
    "    \n",
    "    \n",
    "    #PEPC constraint (Reaction id: PPCc)\n",
    "    #Need to constrain it to 0 since reaction is only detected in Vascular tissue\n",
    "    pepc_BS = model.reactions.PPCc_BS\n",
    "    pepc_M = model.reactions.PPCc_M\n",
    "    \n",
    "    pepc_BS.bounds = (0,0)\n",
    "    pepc_M.bounds = (0,0)\n",
    "\n",
    "    #PPDK constraints (Reaction id: PPDKs) (note that this is found in the chloroplast?) \n",
    "    #Not detected via immunolocalization but enzyme activity is detected\n",
    "\n",
    "    ppdks_BS = model.reactions.PPDKs_BS\n",
    "    ppdks_M = model.reactions.PPDKs_M\n",
    "    ppdkc_BS = model.reactions.PPDKc_BS\n",
    "    ppdkc_M = model.reactions.PPDKc_M\n",
    "    wt_ppdks_cons = model.problem.Constraint(ppdks_BS.flux_expression \n",
    "                                             + ppdks_M.flux_expression\n",
    "                                             + ppdkc_BS.flux_expression\n",
    "                                             + ppdkc_M.flux_expression, \n",
    "                                             lb = 0, ub = wt_ppdk)\n",
    "    wt_ppdks_cons.name = 'wt_ppdks_cons'\n",
    "    model.add_cons_vars(wt_ppdks_cons)\n",
    "    \n",
    "    \n",
    "    #Malate Dehydrogenase \n",
    "    #Only mitochondrial in WT Rice M cells\n",
    "    #Reactions encoded in the model include MDHs, MDHc, MDHx.\n",
    "    model.reactions.MDHs_M.bounds = (0,0)\n",
    "    model.reactions.MDHc_M.bounds = (0,0)\n",
    "    model.reactions.MDHx_M.bounds = (0,0)\n",
    "    model.reactions.MDHs_BS.bounds = (0,0)\n",
    "    model.reactions.MDHc_BS.bounds = (0,0)\n",
    "    model.reactions.MDHx_BS.bounds = (0,0)\n",
    "    model.reactions.MDHm_BS.bounds = (0,0)\n",
    "    #Add constraints to MDHm\n",
    "    mdhm_M = model.reactions.MDHm_M\n",
    "    \n",
    "    wt_mdh_cons = model.problem.Constraint(mdhm_M.flux_expression,\n",
    "                                           lb= -wt_mdh, ub=wt_mdh)\n",
    "    wt_mdh_cons.name = \"wt_mdh_cons\"\n",
    "    model.add_cons_vars(wt_mdh_cons)\n",
    "\n",
    "    \n",
    "    \n",
    "    #NADP-ME (Since no signal is detected in WT, no locational constraints are imposed)\n",
    "    #Let's see if I can force it to have a small amount of flux \n",
    "    mdh2s_M = model.reactions.MDH2s_M\n",
    "    mdh2s_BS = model.reactions.MDH2s_BS\n",
    "    mdh2c_M = model.reactions.MDH2s_M\n",
    "    mdh2c_BS = model.reactions.MDH2s_BS\n",
    "\n",
    "\n",
    "    wt_nadpme_cons = model.problem.Constraint(mdh2s_M.flux_expression\n",
    "                                             + mdh2s_BS.flux_expression\n",
    "                                              + mdh2c_M.flux_expression\n",
    "                                              + mdh2c_BS.flux_expression,\n",
    "                                             lb= 0, ub=wt_nadp_me)\n",
    "    wt_nadpme_cons.name = \"wt_nadpme_cons\"\n",
    "    model.add_cons_vars(wt_nadpme_cons)\n",
    "\n",
    "\n",
    "    #I should add constraints for Carbonic Anhydrase \n",
    "    #I should constrain it to 0.4 ubar, which would constitute ambient CO2 partial pressure\n",
    "    #Flux is reversible so constraints are bi-directional\n",
    "    #This should be revised considering that it allows reversible reactions  and an abnormally high flux thru carbonic anhydrase, which shouldn't be the case\n",
    "\n",
    "    hco3es_m = model.reactions.HCO3Es_M.flux_expression\n",
    "    hco3ec_m = model.reactions.HCO3Ec_M.flux_expression\n",
    "    hco3em_m = model.reactions.HCO3Em_M.flux_expression\n",
    "    hco3es_bs = model.reactions.HCO3Es_BS.flux_expression\n",
    "    hco3ec_bs = model.reactions.HCO3Ec_BS.flux_expression\n",
    "    hco3em_bs = model.reactions.HCO3Em_BS.flux_expression\n",
    "\n",
    "    ca_cons = model.problem.Constraint(hco3es_m + hco3ec_m + hco3em_m \n",
    "                                       + hco3es_bs + hco3ec_bs + hco3em_bs,\n",
    "                                      lb = -wt_CA, ub = wt_CA)\n",
    "    ca_cons.name = 'Carbonic_anhydrase_constraint'\n",
    "    model.add_cons_vars(ca_cons)\n",
    "\n",
    "\n",
    "    #Rbcl constaints\n",
    "    #Retrieve flux expressions oof each RBCl reaction\n",
    "    rbpc_M = model.reactions.RBPCs_M.flux_expression\n",
    "    rbpc_BS = model.reactions.RBPCs_BS.flux_expression\n",
    "    rbpo_M = model.reactions.RBPOs_M.flux_expression\n",
    "    rbpo_BS = model.reactions.RBPOs_BS.flux_expression\n",
    "\n",
    "    #Constraint such that it is limited to 132 umol m-2 s-1\n",
    "    rbcl_vcmax_cons = model.problem.Constraint(rbpc_M + rbpc_BS, lb = 0, ub= 132)\n",
    "    rbcl_vcmax_cons.name='rbcl_vcmax_cons'\n",
    "    model.add_cons_vars(rbcl_vcmax_cons)\n",
    "    #Constraints for rbcl flux such that v_c/v_o = 3 or higher.\n",
    "    rbcl_vcvo = model.problem.Constraint(3*(rbpo_M + rbpo_BS) \n",
    "                                         - 1*(rbpc_M + rbpc_BS),\n",
    "                                         lb=0,ub=1000)\n",
    "    rbcl_vcvo.name = 'rbcl_vc/vo_ratio'\n",
    "    model.add_cons_vars(rbcl_vcvo)\n",
    "\n",
    "    #Turn off the RBPC2s reactions since we already defined the constraints above\n",
    "    model.reactions.RBPC2s_M.bounds = (0,0)\n",
    "    model.reactions.RBPC2s_BS.bounds = (0,0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #What if I simply constrained that of the M cell one to 3:1?\n",
    "    #This constraint is pretty good actually. \n",
    "    #This allows the system to be set at a specific Vc/Vo rate while still allowing local variation \n",
    "    #wherein Rubisco may act in an uncoupled fashion and may have favorable internal vc/vo rates.\n",
    "# #This code block is to set a constraint such that M-to-BS cell NGAM ratio is 10-to-1 \n",
    "# #Similar to what Moreno-Villena et al (2022) had done \n",
    "\n",
    "#This function takes two arguments: the model and the maximal  ppfd input to the system\n",
    "def add_ngam_cons(model, ppfd): \n",
    "    #Turn off other ngam reactions\n",
    "    model.reactions.ngam_atp_s_M.bounds = (0,0)\n",
    "    \n",
    "    model.reactions.ngam_atp_x_M.bounds = (0,0)\n",
    "    model.reactions.ngam_atp_m_M.bounds = (0,0)\n",
    "    \n",
    "    model.reactions.ngam_atp_s_BS.bounds = (0,0)\n",
    "    model.reactions.ngam_atp_x_BS.bounds = (0,0)\n",
    "    model.reactions.ngam_atp_m_BS.bounds = (0,0)\n",
    "    \n",
    "    ngam_atp_m = mm.get_rxn(model, 'ngam_atp_c_M')\n",
    "    ngam_atp_bs = mm.get_rxn(model, 'ngam_atp_c_BS')\n",
    "    \n",
    "    \n",
    "    \n",
    "    ngam_atp_m.bounds = (0,inf)\n",
    "    ngam_atp_bs.bounds = (0,inf)\n",
    "    ngam_ratio = mm.set_fix_flux_ratio({ngam_atp_m.id:10, ngam_atp_bs.id:1}, model)\n",
    "    ngam_ratio.name = 'ngam_BS/M_ratio'\n",
    "    model.add_cons_vars(ngam_ratio)\n",
    "\n",
    "    #Retrieve NGAM reactions\n",
    "    ngam_nadphox_c_M = mm.get_rxn(model, 'ngam_nadphox_c_M')\n",
    "    ngam_nadphox_s_M = mm.get_rxn(model, 'ngam_nadphox_s_M')\n",
    "    ngam_nadphox_m_M = mm.get_rxn(model, 'ngam_nadphox_m_M')\n",
    "    ngam_nadphox_c_BS = mm.get_rxn(model, 'ngam_nadphox_c_BS')\n",
    "    ngam_nadphox_s_BS = mm.get_rxn(model, 'ngam_nadphox_s_BS')\n",
    "    ngam_nadphox_m_BS = mm.get_rxn(model, 'ngam_nadphox_m_BS')\n",
    "\n",
    "\n",
    "    #Set Fixed fluxes\n",
    "    nadphox_c_s_M = mm.set_fix_flux_ratio({ngam_nadphox_c_M.id:1, ngam_nadphox_s_M.id:1},model)\n",
    "    nadphox_c_s_M.name = \"nadphox_cs_ratio_M\"\n",
    "    nadphox_s_m_M = mm.set_fix_flux_ratio({ngam_nadphox_s_M.id:1, ngam_nadphox_m_M.id:1}, model)\n",
    "    nadphox_s_m_M.name = \"nadphox_sm_ratio_M\"\n",
    "\n",
    "    nadphox_c_s_BS = mm.set_fix_flux_ratio({ngam_nadphox_c_BS.id:1, ngam_nadphox_s_BS.id:1},model)\n",
    "    nadphox_c_s_BS.name = \"nadphox_cs_ratio_BS\"\n",
    "    nadphox_s_m_BS = mm.set_fix_flux_ratio({ngam_nadphox_s_BS.id:1, ngam_nadphox_m_BS.id:1}, model)\n",
    "    nadphox_s_m_BS.name = \"nadphox_sm_ratio_BS\"\n",
    "\n",
    "    #Add constraints\n",
    "    model.add_cons_vars(nadphox_c_s_M)\n",
    "    model.add_cons_vars(nadphox_s_m_M)\n",
    "    model.add_cons_vars(nadphox_c_s_BS)\n",
    "    model.add_cons_vars(nadphox_s_m_BS)\n",
    "\n",
    "    #Retrieve flux expressionns\n",
    "    fex_nadphox_c_M =  mm.get_flux_exp(model, ngam_nadphox_c_M)\n",
    "    fex_nadphox_s_M = mm.get_flux_exp(model, ngam_nadphox_s_M)\n",
    "    fex_nadphox_m_M = mm.get_flux_exp(model, ngam_nadphox_m_M)\n",
    "\n",
    "    fex_nadphox_c_BS =  mm.get_flux_exp(model, ngam_nadphox_c_BS)\n",
    "    fex_nadphox_s_BS =  mm.get_flux_exp(model, ngam_nadphox_s_BS)\n",
    "    fex_nadphox_m_BS =  mm.get_flux_exp(model, ngam_nadphox_m_BS)\n",
    "\n",
    "    fex_atp_c_M = mm.get_flux_exp(model, ngam_atp_m)\n",
    "    fex_atp_c_BS =  mm.get_flux_exp(model, ngam_atp_bs)\n",
    "\n",
    "    #Set the constraint between ATP:NADPH NGAM to 3:1\n",
    "    nadphox_atpase = model.problem.Constraint(3*(fex_nadphox_c_M + fex_nadphox_s_M + fex_nadphox_m_M\n",
    "                                                       + fex_nadphox_c_BS + fex_nadphox_s_BS + fex_nadphox_m_BS) \n",
    "                                         - 1*(fex_atp_c_M + fex_atp_c_BS),\n",
    "                                         lb=0,ub=0)\n",
    "    nadphox_atpase.name = \"nadphox_atpase_ratio\"\n",
    "    model.add_cons_vars(nadphox_atpase)\n",
    "    #Compute NGAM value and add constraint as a lower bound/upper bound to model\n",
    "    ngam_value = compute_ngam_atp(ppfd)\n",
    "    ngam_cons = model.problem.Constraint(fex_atp_c_M + \n",
    "                                        fex_atp_c_BS, lb=ngam_value, ub=ngam_value)\n",
    "    ngam_cons.name = 'NGAM_ATP_constraint'\n",
    "    model.add_cons_vars(ngam_cons)\n",
    "    \n",
    "#This code  block gives a snapshot of the relevant fluxes on each of the cell types based on the saved sample_fluxes values above\n",
    "\n",
    "def print_summary(model, sample_fluxes_df):\n",
    "    print('rbcl M cell: ', sample_fluxes['RBPCs_M'], 'rbcl BS cell: ',sample_fluxes['RBPCs_BS'])\n",
    "    print('rbcl M cell (photorespiration)', sample_fluxes['RBPOs_M'], 'rbcl BS cell (PR)', sample_fluxes['RBPOs_BS'])\n",
    "    print('vc/vo M:', sample_fluxes['RBPCs_M']/sample_fluxes['RBPOs_M'], 'vc/vo BS:', sample_fluxes['RBPCs_BS']/sample_fluxes['RBPOs_BS'])\n",
    "    print('RBPC2s_M', sample_fluxes['RBPC2s_M'], 'RBPC2s_BS', sample_fluxes['RBPC2s_BS'])\n",
    "    print('PEPC M', sample_fluxes['PPCc_M'], 'PEPC BS', sample_fluxes['PPCc_BS'])\n",
    "    print('Carbonic Anhydrase (Cytosolic) M', sample_fluxes['HCO3Ec_M'], 'Carbonic Anhydrase (Cytosolic) BS', sample_fluxes['HCO3Ec_BS'])\n",
    "    print('NADP-ME M', sample_fluxes['MDHys_M'], 'NADP-ME BS', sample_fluxes['MDHys_BS'])\n",
    "    print('Biomass M: ', sample_fluxes['Straw_Biomass_M'], 'Biomass BS', sample_fluxes['Straw_Biomass_BS'])\n",
    "    print('Phloem M: ', sample_fluxes['DM_Phloem_M'], 'Phloem BS', sample_fluxes['DM_Phloem_BS'])\n",
    "    print('co2 consumption M', sample_fluxes['CO2tex_M'], 'co2 consumption BS', sample_fluxes['CO2tex_BS'])\n",
    "    print('o2 consumption M', sample_fluxes['O2tex_M'], 'o2 consumption BS', sample_fluxes['O2tex_BS'])\n",
    "    print('Photosystem II M', sample_fluxes['PSIINC_M'], 'PSII BS', sample_fluxes['PSIINC_BS'])\n",
    "    print('PSI M', sample_fluxes['PSIMR_M'], 'PSI BS', sample_fluxes['PSIMR_BS'])\n",
    "    print('PPFD M: ', sample_fluxes['PRISM_white_LED_M'], 'PPFD BS: ', sample_fluxes['PRISM_white_LED_BS'])\n",
    "    print('ATP synthesis (stromal) M', sample_fluxes['ATPSs_M'], 'ATP synthase (mit) M', sample_fluxes['ATPSm_M'])\n",
    "    pd_rxn = [x for x in model.reactions if \"pd\" in x.id and \"h2o\" not in x.id]\n",
    "    pd_abs_flux = 0\n",
    "    for pds in pd_rxn:\n",
    "        pd_abs_flux += abs(sample_fluxes[pds.id])\n",
    "    \n",
    "    print('pd_abs_flux: ', pd_abs_flux)\n",
    "    \n",
    "#initialize list of transgenic reactions to add  to model\n",
    "\n",
    "def add_trans_reactions(model):\n",
    "    '''\n",
    "    This function is used to add a number of new tissue-specific reactions that were not present in the\n",
    "    original model to facilitate modelling of the transgenic C4 rice\n",
    "    '''\n",
    "    trans_list = list()\n",
    "    #Transgenic PEPC copy\n",
    "    #PEPC = Chloroplastic in M & V (rxn id: PPCc)\n",
    "    trans_ppcs = Reaction('trans_PPCs_M')\n",
    "    trans_ppcs.name = \"Phosphoenolpyruvate carboxylase, plastidic (Transgenic)\"\n",
    "    \n",
    "    pep_s0 = model.metabolites.pep_s0\n",
    "    hco3_s0 = model.metabolites.hco3_s0\n",
    "    oaa_s0 = model.metabolites.oaa_s0\n",
    "    pi_s0 = model.metabolites.pi_s0\n",
    "\n",
    "\n",
    "    #Add metabolites, bounds, and subsystem\n",
    "    trans_ppcs.add_metabolites({hco3_s0:-1, pep_s0:-1, oaa_s0:1, pi_s0:1})\n",
    "    trans_ppcs.bounds= model.reactions.PPCc_M.bounds\n",
    "    trans_ppcs.subsystem = model.reactions.PPCc_M.subsystem\n",
    "\n",
    "    trans_list.append(trans_ppcs)\n",
    "\n",
    "\n",
    "    #Transgenic PPDK Copy\n",
    "    trans_ppdks_m = Reaction('trans_PPDKs_M')\n",
    "    trans_ppdks_m.add_metabolites(model.reactions.PPDKs_M.metabolites)\n",
    "    trans_ppdks_m.bounds = model.reactions.PPDKs_M.bounds\n",
    "    trans_ppdks_m.name = \"Pyruvate phosphate dikinase, plastidic (Transgenic)\"\n",
    "\n",
    "    trans_ppdks_bs = Reaction('trans_PPDKs_BS')\n",
    "    trans_ppdks_bs.add_metabolites(model.reactions.PPDKs_BS.metabolites)\n",
    "    trans_ppdks_bs.bounds = model.reactions.PPDKs_BS.bounds\n",
    "    trans_ppdks_bs.name = \"Pyruvate phosphate dikinase, plastidic (Transgenic)\"\n",
    "\n",
    "    trans_list.append(trans_ppdks_m)\n",
    "    trans_list.append(trans_ppdks_bs)\n",
    "\n",
    "    #Transgenic NADP-ME\n",
    "    #NADP-ME = Mitochondrial in M\n",
    "    trans_nadp_me = Reaction('trans_MDH2m_M')\n",
    "\n",
    "    #retrieve reactants\n",
    "    mal_m0 = model.metabolites.get_by_id('mal-L_m0')\n",
    "    nadp_m0 = model.metabolites.nadp_m0\n",
    "    co2_m0 = model.metabolites.co2_m0\n",
    "    nadph_m0 = model.metabolites.nadph_m0\n",
    "    pyr_m0 = model.metabolites.pyr_m0\n",
    "\n",
    "    #Add to rxn\n",
    "    trans_nadp_me.add_metabolites({mal_m0:-1, nadp_m0:-1, co2_m0:1, nadph_m0:1, pyr_m0:1})\n",
    "    #Add bounds\n",
    "    trans_nadp_me.bounds=(-inf, inf)\n",
    "\n",
    "    trans_list.append(trans_nadp_me)\n",
    "\n",
    "\n",
    "    #Trans CA\n",
    "    #Cytosolic in M\n",
    "    trans_hco3ec_M = Reaction('trans_hco3ec_M')\n",
    "    trans_hco3ec_M.name = 'carbonic anhydrase, cytosolic'\n",
    "    trans_hco3ec_M.add_metabolites(model.reactions.HCO3Ec_M.metabolites)\n",
    "    trans_hco3ec_M.bounds = model.reactions.HCO3Ec_M.bounds\n",
    "\n",
    "    trans_hco3ec_M.subsystem = model.reactions.HCO3Ec_M.subsystem\n",
    "    trans_list.append(trans_hco3ec_M)\n",
    "\n",
    "\n",
    "    #Bulk add to model\n",
    "    model.add_reactions(trans_list)\n",
    "    \n",
    "    model.repair()\n",
    "####ADDING TRANS CONSTRAINTS\n",
    "\n",
    "def add_trans_constraints(model,\n",
    "                         trans_pepc_rates = 7.01,\n",
    "                         trans_ppdks_rates = 3.66,\n",
    "                         trans_mdh_rates = 152.87,\n",
    "                         trans_nadp_me_rates = 0.60,\n",
    "                         trans_CA_rates = 8):\n",
    "    '''\n",
    "    This function is used to add another layer of constraints to parametize model based on the\n",
    "    Enzyme reaction rates assayed from Ermakova et al (2021) where the locations are based on the \n",
    "    each of the transgenic enzyme's tissue-specific localizations. \n",
    "    '''\n",
    "    \n",
    "    #PEPC constraint\n",
    "    wt_PPCc_M = mm.get_rxn(model, 'PPCc_M')\n",
    "    wt_PPCc_BS = mm.get_rxn(model, 'PPCc_BS')\n",
    "    trans_PPCs_M = mm.get_rxn(model, 'trans_PPCs_M')                           \n",
    "    trans_PEPC_cons = model.problem.Constraint(trans_PPCs_M.flux_expression\n",
    "                                            +wt_PPCc_BS.flux_expression \n",
    "                                            + wt_PPCc_M.flux_expression, \n",
    "                                            lb = 0, ub = trans_pepc_rates)\n",
    "\n",
    "    model.add_cons_vars(trans_PEPC_cons)\n",
    "\n",
    "    #PPDK constraint\n",
    "    trans_PPDKs_M  = mm.get_rxn(model, 'trans_PPDKs_M')\n",
    "    trans_PPDKs_BS = mm.get_rxn(model, 'trans_PPDKs_BS')\n",
    "    wt_PPDKs_M = mm.get_rxn(model, 'PPDKs_M')\n",
    "    wt_PPDKs_BS = mm.get_rxn(model, 'PPDKs_BS')\n",
    "    \n",
    "    trans_PPDKs_cons = model.problem.Constraint( \n",
    "        trans_PPDKs_BS.flux_expression + trans_PPDKs_M.flux_expression \n",
    "        +wt_PPDKs_BS.flux_expression + wt_PPDKs_M.flux_expression, \n",
    "                                             lb = 0, ub = trans_ppdks_rates)\n",
    "    trans_PPDKs_cons.name = 'trans_ppdks_cons'\n",
    "    model.add_cons_vars(trans_PPDKs_cons)\n",
    "    \n",
    "\n",
    "\n",
    "    #Malate Dehydrogenase Constraints\n",
    "    trans_MDHm_M = mm.get_rxn(model, 'MDHm_M')\n",
    "    trans_MDHs_M = mm.get_rxn(model, 'MDHs_M')\n",
    "    trans_MDHs_BS = mm.get_rxn(model, 'MDHs_BS')\n",
    "    \n",
    "    #Change bounds to reflect the Trans state (Based on Immunoblotting)\n",
    "    trans_MDHm_M.bounds = (-inf, inf)\n",
    "    trans_MDHs_M.bounds = (-inf, inf)\n",
    "    trans_MDHs_BS.bounds = (-inf, inf)\n",
    "    \n",
    "    trans_mdh_cons =  model.problem.Constraint(\n",
    "       trans_MDHm_M.flux_expression + \n",
    "        trans_MDHs_M.flux_expression + \n",
    "        trans_MDHs_BS.flux_expression, \n",
    "        lb= -trans_mdh_rates, ub=trans_mdh_rates)\n",
    "\n",
    "    trans_mdh_cons.name = \"trans_mdh_cons\"\n",
    "    model.add_cons_vars(trans_mdh_cons)\n",
    "\n",
    "    \n",
    "    \n",
    "    #Add NADP-ME constraints\n",
    "    trans_MDH2m_M = mm.get_rxn(model, 'trans_MDH2m_M')\n",
    "    wt_MDH2s_M = mm.get_rxn(model, 'MDH2s_M')\n",
    "    wt_MDH2s_BS = mm.get_rxn(model, 'MDH2s_BS')\n",
    "    \n",
    "    \n",
    "    \n",
    "    trans_nadpme_cons = model.problem.Constraint(\n",
    "        trans_MDH2m_M.flux_expression + \n",
    "        wt_MDH2s_M.flux_expression + \n",
    "        wt_MDH2s_BS.flux_expression,\n",
    "        lb= -trans_nadp_me_rates, ub=trans_nadp_me_rates)\n",
    "    \n",
    "    trans_nadpme_cons.name = \"trans_nadpme\"\n",
    "    model.add_cons_vars(trans_nadpme_cons)\n",
    "\n",
    "    #Add carbonic anhydrase constraints\n",
    "\n",
    "    trans_hco3ec_M = mm.get_rxn(model, 'trans_hco3ec_M')\n",
    "    wt_hco3ec_M = mm.get_rxn(model, 'HCO3Ec_M')\n",
    "    wt_hco3em_M = mm.get_rxn(model, 'HCO3Em_M')\n",
    "    wt_hco3es_M = mm.get_rxn(model, 'HCO3Es_M')\n",
    "    wt_hco3ec_BS = mm.get_rxn(model, 'HCO3Ec_BS')\n",
    "    wt_hco3em_BS = mm.get_rxn(model, 'HCO3Em_BS')\n",
    "    wt_hco3es_BS = mm.get_rxn(model, 'HCO3Es_BS')\n",
    "    \n",
    "    trans_ca_cons = model.problem.Constraint(trans_hco3ec_M.flux_expression + \n",
    "                                             wt_hco3es_M.flux_expression + \n",
    "                                             wt_hco3ec_M.flux_expression + \n",
    "                                             wt_hco3em_M.flux_expression + \n",
    "                                             wt_hco3es_BS.flux_expression + \n",
    "                                             wt_hco3ec_BS.flux_expression + \n",
    "                                             wt_hco3em_BS.flux_expression,\n",
    "                                      lb = -trans_CA_rates, ub = trans_CA_rates)\n",
    "    trans_ca_cons.name = 'Trans_CA_cons'\n",
    "    model.add_cons_vars(trans_ca_cons)\n",
    "    model.repair()\n",
    "    \n",
    "\n",
    "\n",
    "#Read 2-cell model\n",
    "wt_model = cobra.io.read_sbml_model(\"./model/ios2164_2cell.xml\")\n",
    "trans_model = cobra.io.read_sbml_model(\"./model/ios2164_2cell.xml\")\n",
    "\n",
    "wt_model.solver = 'gurobi'\n",
    "trans_model.solver = 'gurobi'\n",
    "\n",
    "\n",
    "trans_model\n",
    "add_trans_reactions(trans_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d08460-95dc-4dea-ae6e-a7c85ca4122e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_flux_samples(model, num_samples, batch_size, output_filename,thinning,processes=7, nproj=None,output_dir='./flux_results/flux_sampling/'):\n",
    "    #This function is used to initialize a flux sampler and afterwards generate a csv file containing the sample solutions.\n",
    "    #default batch size is 1000\n",
    "\n",
    "    #Generate sampler\n",
    "    print(\"generating sampler for model\")\n",
    "    sampler = sampling.OptGPSampler(model, processes=processes, thinning=thinning, nproj=nproj)\n",
    "    print(\"done generating OPTGP sampler\")\n",
    "    \n",
    "    \n",
    "    #Define output file\n",
    "    output_dir = str(output_dir)\n",
    "    output_filename = str(output_filename)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    print('saving output to ', f\"{output_dir}/{output_filename}\")\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(num_samples // batch_size):\n",
    "        print(f\"Generating batch {i+1}/{num_samples//batch_size}\")\n",
    "        samples = sampler.sample(n=batch_size)\n",
    "        df = pd.DataFrame(samples, columns=model.reactions.list_attr(\"id\"))\n",
    "        if i == 0:\n",
    "            df.to_csv(f\"{output_dir}/{output_filename}\", index=False)\n",
    "        else:\n",
    "            df.to_csv(f\"{output_dir}/{output_filename}\", index=False, header=False, mode=\"a\")\n",
    "        \n",
    "    \n",
    "\n",
    "def parametrize_model(model, ppfd_low, ppfd_high, co2,if_trans, loopless=False, frac_optimum=1, pruning=False, fva_bounds_file='', intermediate_fva_results='', save_final_bounds_file=''):\n",
    "    \n",
    "    \n",
    "    model_instance=model.copy()\n",
    "    model_instance.tolerance = 1e-9\n",
    "\n",
    "\n",
    "    print('start time for parametrize_model(): ' ,datetime.datetime.now())\n",
    "    model_instance.medium = define_model_medium(model_instance, co2=co2, o2=inf, ppfd=inf, h=inf, nh4=inf, no3=inf)\n",
    "    turn_off_cofac_cycles(model_instance)\n",
    "    add_tissue_constraints(model_instance)\n",
    "    add_enzyme_constraints(model_instance)\n",
    "\n",
    "    #Adds NGAM (Computed as an average between the high and low instead of directly constraining it to the model, w/c makes the problem non-linear)\n",
    "    add_ngam_cons(model_instance, (ppfd_high+ppfd_low)/2)\n",
    "\n",
    "    #Constrain PPFD range to indicated value\n",
    "    model_instance.reactions.get_by_id('EX_photonVis(e)').bounds = (-1*ppfd_high, -1*ppfd_low)\n",
    "\n",
    "\n",
    "    #Check if trans then add constraints if true\n",
    "    if if_trans==True:\n",
    "        add_trans_reactions(model_instance)\n",
    "        add_trans_constraints(model_instance)\n",
    "\n",
    "\n",
    "    #Readd objective coefficient, maybe it'll work?\n",
    "    model_instance.reactions.get_by_id('DM_Phloem_BS').objective_coefficient = 1\n",
    "\n",
    "    '''Run FVA to preprocess the model to fix reaction reversibilities as well as to ensure that there are no \"extreme\" fluxes in the final sampling\n",
    "     Perform Flux Variability Analysis (FVA)\n",
    "        This step constrains the upper and lower bounds to the detected \"Maximal\" and \"minimal\" fluxes given an objective\n",
    "        The default fraction of optimum will be implemented\n",
    "    Will implement pFBA factor to constrain the model to 110% of the detected lowest net fluxes'''\n",
    "\n",
    "\n",
    "    list_infeasibles = list()\n",
    "\n",
    "    #flux_variability_analysis produces a Pandas Dataframe that can be taken apart and applied to the model_instance as direct bounds. \n",
    "\n",
    "    \n",
    "    #This function reads the previous bounds to see whether i\n",
    "    if fva_bounds_file:\n",
    "        print('reading previous FVA bounds')\n",
    "        fva_result = pd.read_csv(fva_bounds_file, index_col=0)\n",
    "        fva_result.columns = ['minimum', 'maximum']\n",
    "\n",
    "        # Create a list to store the reactions that need to be removed\n",
    "        reactions_to_remove = []\n",
    "\n",
    "        # Add reactions that are not in the fva_bounds_file to the rerunning FVA\n",
    "        for rxn in model_instance.reactions:\n",
    "            if rxn.id not in fva_result.index:\n",
    "                # print(f'{rxn.id} not in previous bounds. Removing..')\n",
    "                reactions_to_remove.append(rxn.id)\n",
    "            else:\n",
    "                if (fva_result.loc[rxn.id, 'minimum'] == 0) and (fva_result.loc[rxn.id, 'maximum'] == 0):\n",
    "                    # print(f'{rxn.id} has 0/0 bounds in the previous result. Removing..')\n",
    "                    reactions_to_remove.append(rxn.id)\n",
    "        \n",
    "        print(f'Reactions to remove: {len(reactions_to_remove)}')\n",
    "        # Remove reactions from the model_instance\n",
    "        model_instance.remove_reactions(reactions_to_remove)\n",
    "\n",
    "        for i in reactions_to_remove:\n",
    "            fva_result = fva_result.drop(index=i)\n",
    "                \n",
    "            \n",
    "            \n",
    "    else:\n",
    "        print('computing Loopless FVA to model from scratch')\n",
    "        fva_result = cobra.flux_analysis.flux_variability_analysis(model_instance, loopless=loopless, fraction_of_optimum=frac_optimum, pfba_factor=1.1, processes=7)\n",
    "\n",
    "\n",
    "    # Set FVA constraints in the model\n",
    "    print('setting FVA constraints to model')\n",
    "\n",
    "\n",
    "    for reaction_id, bounds in fva_result.iterrows():\n",
    "        reaction = model_instance.reactions.get_by_id(reaction_id)        \n",
    "\n",
    "\n",
    "\n",
    "        #This method instead double checks the model per flux range and checks if it returns an infeasible solution/0 objective. \n",
    "        #First save old bounds \n",
    "        old_bounds = reaction.bounds\n",
    "\n",
    "        # Check which is higher or lower to avoid Value errors\n",
    "        lower_bound = float(min(bounds['minimum'], bounds['maximum']))\n",
    "        upper_bound = float(max(bounds['minimum'], bounds['maximum']))\n",
    "\n",
    "        #Check reaction if it is unbounded then append to list_infeasibles for rerunning\n",
    "        if lower_bound==-1e6 or upper_bound==1e6 or np.nan in (lower_bound, upper_bound):\n",
    "            # reaction.bounds = old_bounds\n",
    "            print(f'reaction {reaction_id} is unbounded/nan. Adding to rerun list. {lower_bound}{upper_bound}')\n",
    "            list_infeasibles.append(reaction)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            reaction.bounds = (lower_bound, upper_bound)\n",
    "        \n",
    "        except AttributeError: #For catching NaNs\n",
    "            print(f'reaction {reaction_id} is has nan bounds. Adding to rerun list. {lower_bound}{upper_bound}')\n",
    "            reaction.bounds = old_bounds\n",
    "            list_infeasibles.append(reaction)\n",
    "            continue\n",
    "        \n",
    "        \n",
    "#         else:\n",
    "#             try:\n",
    "#                 #Fix the upper and lower bounds\n",
    "#                 reaction.bounds = (lower_bound, upper_bound)\n",
    "\n",
    "#                 #Generate solution and status for checking if model becomes infeasible due to the bounds. Otherwise recompute it\n",
    "#                 solution = model_instance.slim_optimize()\n",
    "#                 status= model_instance.optimize().status\n",
    "\n",
    "\n",
    "\n",
    "#                 #Revert model to previous flux bounds in case it causes feasibility issues\n",
    "#                 if status=='infeasible' or solution==0: #If it causes a 0 obj. function then it's considered broke\n",
    "#                     if reaction_id== 'DM_Phloem_BS':\n",
    "#                         print(f'objective bounds:{lower_bound}{upper_bound}')\n",
    "#                         continue\n",
    "\n",
    "\n",
    "#                     print(f'{reaction_id} causes infeasible status/0 objective!')\n",
    "#                     reaction.bounds=old_bounds\n",
    "#                     list_infeasibles.append(reaction)\n",
    "#                     continue\n",
    "\n",
    "#             except AttributeError:\n",
    "#                 print(f'reaction {reaction_id} is unbounded/nan. Adding to rerun list. {lower_bound}{upper_bound}')\n",
    "#                 reaction.bounds = old_bounds\n",
    "#                 list_infeasibles.append(reaction)\n",
    "#                 continue\n",
    "\n",
    "        #I think this does more harm than good.\n",
    "\n",
    "\n",
    "    print('Checking done. Length of reactions that cause infeasibility issues/unbounded: ', len(list_infeasibles))\n",
    "\n",
    "\n",
    "    #Save intermediate FVA results (For debugging)\n",
    "    if intermediate_fva_results != '':\n",
    "        save_model_bounds(model_instance, f'{intermediate_fva_results}_before_pruning.csv', directory='./flux_results/flux_sampling/model_bounds/for_debugging/07292023')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Checks if there are any infeasible solutions in the model then reruns the non-loopless version on the NaNs list. This in turn reruns the model to use the non-loopless version of the algorithm instead. It would at least relax the bounds set by the loopless option\n",
    "    #To bounds that are still feasible but still constrained somewhat (rather than setting it to a large value). The rationale behind this is that the first LL-FVA iteration already constrains the model to remove most loops while this iteration would ensure that the model returns a feasible solution\n",
    "\n",
    "        #Rerun FVA\n",
    "    if len(list_infeasibles) != 0:\n",
    "        print('recomputing list_infeasibles')\n",
    "        fva_non_loopless =  cobra.flux_analysis.flux_variability_analysis(model_instance, reaction_list=list_infeasibles, loopless=True ,fraction_of_optimum=frac_optimum, pfba_factor=1.1, processes=7)\n",
    "\n",
    "        for reaction_id, bounds in fva_non_loopless.iterrows():\n",
    "            reaction = model_instance.reactions.get_by_id(reaction_id)        \n",
    "            # Check which is higher or lower to avoid Value errors\n",
    "            lower_bound = float(min(bounds['minimum'], bounds['maximum']))\n",
    "            upper_bound = float(max(bounds['minimum'], bounds['maximum']))\n",
    "            print(f'setting bounds for rerun reaction {reaction_id}. Bounds: {lower_bound} | {upper_bound}')\n",
    "            reaction.bounds = (lower_bound, upper_bound)\n",
    "\n",
    "    #Intermediate check objective\n",
    "    print('printing model objective:')\n",
    "    print(model_instance.optimize())\n",
    "\n",
    "\n",
    "    if pruning==True:\n",
    "        blocked_reactions =  list()\n",
    "        for reactions in model_instance.reactions:\n",
    "            if reactions.bounds == (0,0):\n",
    "                blocked_reactions.append(reactions.id)\n",
    "        print('reactions number (before pruning): ', len(model_instance.reactions))\n",
    "        model_instance.remove_reactions(blocked_reactions)\n",
    "        print('reactions number (after pruning): ', len(model_instance.reactions))\n",
    "\n",
    "    # Identify unneeded metabolites\n",
    "    unneeded_metabolites = []\n",
    "    for metabolite in model.metabolites:\n",
    "        if metabolite.reactions == []:\n",
    "            unneeded_metabolites.append(metabolite)\n",
    "\n",
    "    # Remove unneeded metabolites from the model\n",
    "    model.remove_metabolites(unneeded_metabolites)\n",
    "\n",
    "\n",
    "\n",
    "    #This ensures the model is feasible before passing to the solver (Infeasible models cannot generate any solutions)\n",
    "    print('final model objective (Final feasibility check):')\n",
    "    print(model_instance.optimize())\n",
    "\n",
    "    print('end time for parametrize_model(): ' ,datetime.datetime.now())\n",
    "    \n",
    "    if save_final_bounds_file:\n",
    "        \n",
    "        save_model_bounds(model_instance, f'{save_final_bounds_file}_before_sampling.csv')\n",
    "\n",
    "\n",
    "    return model_instance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_model_bounds(model, filename, directory='./flux_results/flux_sampling/model_bounds/'):\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Get the bounds of each reaction in the model\n",
    "    bounds = []\n",
    "    for reaction in model.reactions:\n",
    "        bounds.append([reaction.id, reaction.bounds[0], reaction.bounds[1]])\n",
    "\n",
    "    # Create a DataFrame with the bounds\n",
    "    bounds_df = pd.DataFrame(bounds, columns=['Reaction', 'Lower Bound', 'Upper Bound'])\n",
    "\n",
    "    # Write the DataFrame to a CSV file\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    bounds_df.to_csv(filepath, index=False)\n",
    "\n",
    "    print(f\"Model bounds saved to: {filepath}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf1896b-310d-4003-83a7-ef7d537fe7d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# '''June 22, 2023\n",
    "# Let's try running this script with the following parameters in mind. Hopefully it doesn't result with infeasibility issues anymore.\n",
    "\n",
    "# '''\n",
    "\n",
    "# #Let's try running this script with the following parameters in mind. Hopwefully this doesn't result with infeasibility issues anymore\n",
    "# #I suspect that the model is overconstrained due to the frac_optimum \n",
    "# WT_250_test =  parametrize_model(wt_model, 225,275,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True)\n",
    "\n",
    "# #Generate list of models for flux sampling\n",
    "# sampling_list = [WT_250_test]\n",
    "\n",
    "# print('generating flux samples for parametrized models')\n",
    "# for model in sampling_list:\n",
    "#     print('start time for sampler generation: ' ,datetime.datetime.now())\n",
    "#     model_name = str([k for k, v in locals().items() if v == model][0])\n",
    "#     name_for_file = str('flux_sample_'+model_name+'_relaxed_Loopless_FVA.csv')\n",
    "    \n",
    "#     #Output the flux bounds of the model to ./fva_bounds/name_for_file\n",
    "#     save_model_bounds(model, filename=name_for_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print('Starting OPTGP sampler:')\n",
    "#     generate_flux_samples(model, num_samples =500, batch_size=500, thinning=1, output_filename=name_for_file)\n",
    "#     del model #Deletes model instance to free up memory \n",
    "#     print('end time: ', datetime.datetime.now())\n",
    "    \n",
    "    \n",
    "# '''June 23, 2023\n",
    "# The modifications I've done works marvelously. It even fixes yung consumption ng inorganic nutrients, which was unobservable in the prior attempts. However run times are fairly large so I need to account for that.\n",
    "# Some of my modifications include the following:\n",
    "# setting the fraction optimum to 90% of optimum increasing the solution space for the inputs and outputs\n",
    "# Setting the initial run as loopless then taking into account the reactions with infeasible solutions, and afterwards setting it to non-loopless\n",
    "\n",
    "# Pruning the reaction set to account only for reactions with non-zero flux bounds\n",
    "\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca406088-5276-4af4-b78f-fbb3a3a0432c",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/articulatus/anaconda3/envs/RiceMM/lib/python3.10/site-packages/cobra/core/group.py:147: UserWarning: need to pass in a list\n",
      "  warn(\"need to pass in a list\")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Coleoptile_Biomass_M'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#Let's check if it works as a separate instance per WT and TR?\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wt_model \u001b[38;5;28;01mas\u001b[39;00m wt_model:\n\u001b[0;32m----> 8\u001b[0m     WT_250 \u001b[38;5;241m=\u001b[39m  \u001b[43mparametrize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m225\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m275\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mco2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m29\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_trans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrac_optimum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.90\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloopless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpruning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfva_bounds_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./flux_results/flux_sampling/model_bounds/Modified-Loopless-FVA/WT_250.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Done\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     WT_750 \u001b[38;5;241m=\u001b[39m parametrize_model(wt_model, \u001b[38;5;241m725\u001b[39m, \u001b[38;5;241m775\u001b[39m,co2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m29\u001b[39m, if_trans\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, frac_optimum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.90\u001b[39m, loopless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, pruning\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fva_bounds_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./flux_results/flux_sampling/model_bounds/Modified-Loopless-FVA/WT_750.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m     WT_1500 \u001b[38;5;241m=\u001b[39m parametrize_model(wt_model, \u001b[38;5;241m1475\u001b[39m, \u001b[38;5;241m1525\u001b[39m,co2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m29\u001b[39m, if_trans\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, frac_optimum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.90\u001b[39m, loopless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, pruning\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fva_bounds_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./flux_results/flux_sampling/model_bounds/Modified-Loopless-FVA/WT_1500.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 104\u001b[0m, in \u001b[0;36mparametrize_model\u001b[0;34m(model, ppfd_low, ppfd_high, co2, if_trans, loopless, frac_optimum, pruning, fva_bounds_file, intermediate_fva_results, save_final_bounds_file)\u001b[0m\n\u001b[1;32m    101\u001b[0m     model_instance\u001b[38;5;241m.\u001b[39mremove_reactions(reactions_to_remove)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m reactions_to_remove:\n\u001b[0;32m--> 104\u001b[0m         fva_result \u001b[38;5;241m=\u001b[39m \u001b[43mfva_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomputing Loopless FVA to model from scratch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/RiceMM/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/RiceMM/lib/python3.10/site-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5405\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5406\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/RiceMM/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/RiceMM/lib/python3.10/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/RiceMM/lib/python3.10/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/RiceMM/lib/python3.10/site-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Coleoptile_Biomass_M'] not found in axis\""
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "\n",
    "#Parametrize models per type by generating model instance\n",
    "\n",
    "print('Generating parametrized models for flux sampling')\n",
    "#Let's check if it works as a separate instance per WT and TR?\n",
    "with wt_model as wt_model:\n",
    "    WT_250 =  parametrize_model(wt_model, 225,275,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True, fva_bounds_file='./flux_results/flux_sampling/model_bounds/Modified-Loopless-FVA/WT_250.csv') #Done\n",
    "    WT_750 = parametrize_model(wt_model, 725, 775,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True, fva_bounds_file='./flux_results/flux_sampling/model_bounds/Modified-Loopless-FVA/WT_750.csv')\n",
    "    WT_1500 = parametrize_model(wt_model, 1475, 1525,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True, fva_bounds_file='./flux_results/flux_sampling/model_bounds/Modified-Loopless-FVA/WT_1500.csv')\n",
    "\n",
    "    TR_250 = parametrize_model(wt_model, 225, 275,co2=22.2, if_trans=True, frac_optimum=0.90, loopless=True, pruning=True, fva_bounds_file='./flux_results/flux_sampling/model_bounds/Modified-Loopless-FVA/TR_250.csv')\n",
    "    TR_750 = parametrize_model(wt_model, 725, 775,co2=22.2, if_trans=True, frac_optimum=0.90, loopless=True, pruning=True, fva_bounds_file='./flux_results/flux_sampling/model_bounds/Modified-Loopless-FVA/TR_750.csv')\n",
    "    TR_1500 = parametrize_model(wt_model, 1475, 1525,co2=22.2, if_trans=True, frac_optimum=0.90, loopless=True, pruning=True, fva_bounds_file='./flux_results/flux_sampling/model_bounds/Modified-Loopless-FVA/TR_1500.csv')\n",
    "\n",
    "#Generate list of models for flux sampling\n",
    "sampling_list = [WT_250, WT_750, WT_1500, TR_250, TR_750, TR_1500]\n",
    "\n",
    "print('generating flux samples for parametrized models')\n",
    "for model in sampling_list:\n",
    "    print('start time for sampler generation: ' ,datetime.datetime.now())\n",
    "    model_name = str([k for k, v in locals().items() if v == model][0])\n",
    "    name_for_file = str('flux_sample_'+model_name+'_Relaxed_loopless_FVA_100kT.csv')\n",
    "    #Output the flux bounds of the model to ./fva_bounds/name_for_file\n",
    "    save_model_bounds(model, filename=name_for_file)\n",
    "    \n",
    "    \n",
    "    #Parametrize OPTGP sampler\n",
    "    num_samples =10000\n",
    "    thinning = 100000\n",
    "    \n",
    "    print('Starting OPTGP sampler:')\n",
    "    print('num_samples: ', num_samples)\n",
    "    print('Thinning coefficient: ', thinning)\n",
    "    generate_flux_samples(model, num_samples =num_samples, nproj=1, batch_size=5000, thinning=thinning, output_filename=name_for_file)\n",
    "    del model #Deletes model instance to free up memory \n",
    "    print('end time: ', datetime.datetime.now())\n",
    "\n",
    "#This outputs a set of bounds that we can reuse to re-parametrize a model instead of re-running the parametrization script again.\n",
    "#June 24, 2023 \n",
    "\n",
    "#For some reason the Trans model returns an infeasible solution. I'll check specifically what the problem is \n",
    "#I checked and some reactions cause numerical issues. I can circumvent that by rerunning the loopless FVA function on the problematic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cdc65e-133f-46a3-a87a-3bc887d52604",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%timeit\n",
    "\n",
    "#Need to Rerun WT750 and WT1500 due to internal looping issues. Fixed the parametrization script to catch any unexpected errors in the run. We will not be using any previous rerun results for this\n",
    "\n",
    "#Parametrize models per type by generating model instance\n",
    "\n",
    "print('Generating parametrized models for flux sampling')\n",
    "#Let's check if it works as a separate instance per WT and TR?\n",
    "with wt_model as wt_model:\n",
    "    WT_750 = parametrize_model(wt_model, 725, 775,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True, intermediate_fva_results='WT_750_rerun.csv')\n",
    "    WT_1500 = parametrize_model(wt_model, 1475, 1525,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True, intermediate_fva_results='.WT_1500_rerun.csv')\n",
    "\n",
    "#Generate list of models for flux sampling\n",
    "sampling_list = [WT_750, WT_1500]\n",
    "\n",
    "print('generating flux samples for parametrized models')\n",
    "for model in sampling_list:\n",
    "    print('start time for sampler generation: ' ,datetime.datetime.now())\n",
    "    model_name = str([k for k, v in locals().items() if v == model][0])\n",
    "    name_for_file = str('flux_sample_'+model_name+'_Relaxed_loopless_FVA_100kT_reran.csv')\n",
    "    #Output the flux bounds of the model to ./fva_bounds/name_for_file\n",
    "    save_model_bounds(model, filename=name_for_file)\n",
    "    \n",
    "    \n",
    "    #Parametrize OPTGP sampler\n",
    "    num_samples =10000\n",
    "    thinning = 100000\n",
    "    \n",
    "    print('Starting OPTGP sampler:')\n",
    "    print('num_samples: ', num_samples)\n",
    "    print('Thinning coefficient: ', thinning)\n",
    "    generate_flux_samples(model, num_samples =num_samples, nproj=1, batch_size=5000, thinning=thinning, output_filename=name_for_file)\n",
    "    del model #Deletes model instance to free up memory \n",
    "    print('end time: ', datetime.datetime.now())\n",
    "    \n",
    "    \n",
    "#No more unbounded reactions ///\n",
    "\n",
    "#Flux sampling done. Everything seems in order now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3faa42-8ec1-4733-b76c-9d6290125013",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#JULY 28 2023\n",
    "#Rerunning the sampler script due to an issue with Reaction encoding in the original model.\n",
    "\n",
    "%timeit\n",
    "\n",
    "#Parametrize models per type by generating model instance\n",
    "\n",
    "print('Generating parametrized models for flux sampling')\n",
    "#Let's check if it works as a separate instance per WT and TR?\n",
    "with wt_model as wt_model:\n",
    "    WT_250 =  parametrize_model(wt_model, 225,275,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True, intermediate_fva_results='wt_250_revised_enzyme_cons', save_final_bounds_file='wt_250')\n",
    "    WT_750 = parametrize_model(wt_model, 725, 775,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True, intermediate_fva_results='wt_750_revised_enzyme_cons',save_final_bounds_file='wt_750')\n",
    "    WT_1500 = parametrize_model(wt_model, 1475, 1525,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True, intermediate_fva_results='wt_1500_revised_enzyme_cons', save_final_bounds_file='wt_1500')\n",
    "\n",
    "    TR_250 = parametrize_model(wt_model, 225, 275,co2=22.2, if_trans=True, frac_optimum=0.90, loopless=True, pruning=True, intermediate_fva_results='tr_250_revised_enzyme_cons',save_final_bounds_file='tr_250')\n",
    "    TR_750 = parametrize_model(wt_model, 725, 775,co2=22.2, if_trans=True, frac_optimum=0.90, loopless=True, pruning=True, intermediate_fva_results='tr_750_revised_enzyme_cons',save_final_bounds_file='tr_750')\n",
    "    TR_1500 = parametrize_model(wt_model, 1475, 1525,co2=22.2, if_trans=True, frac_optimum=0.90, loopless=True, pruning=True, intermediate_fva_results='tr_1500_revised_enzyme_cons', save_final_bounds_file='tr_1500')\n",
    "\n",
    "#Generate list of models for flux sampling\n",
    "sampling_list = [WT_250, WT_750, WT_1500, TR_250, TR_750, TR_1500]\n",
    "\n",
    "print('generating flux samples for parametrized models')\n",
    "for model in sampling_list:\n",
    "    print('start time for sampler generation: ' ,datetime.datetime.now())\n",
    "    model_name = str([k for k, v in locals().items() if v == model][0])\n",
    "    name_for_file = str('flux_sample_'+model_name+'_updated_model_cons.csv')\n",
    "    #Output the flux bounds of the model to ./fva_bounds/name_for_file\n",
    "    save_model_bounds(model, filename=name_for_file)\n",
    "    \n",
    "    \n",
    "    #Parametrize OPTGP sampler\n",
    "    num_samples =10000\n",
    "    thinning = 100000\n",
    "    \n",
    "    print('Starting OPTGP sampler:')\n",
    "    print('num_samples: ', num_samples)\n",
    "    print('Thinning coefficient: ', thinning)\n",
    "    generate_flux_samples(model, num_samples =num_samples, batch_size=10000, thinning=thinning, output_filename=name_for_file)\n",
    "    del model #Deletes model instance to free up memory \n",
    "    print('end time: ', datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5eee51-9151-41d4-b805-f384472a4b4f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #JULY 28 2023\n",
    "# #Rerunning the sampler script due to an issue with Reaction encoding in the original model.\n",
    "\n",
    "\n",
    "# #JUly 29, 2023.\n",
    "# #Rerunning the script since for some reason hindi nassave yung bounds dun sa actual variable file when I used a model instance. Stupid ass \n",
    "# %timeit\n",
    "\n",
    "# #Parametrize models per type by generating model instance\n",
    "\n",
    "# print('Generating parametrized models for flux sampling')\n",
    "\n",
    "# #For some reason it causes numerical issues if I don't isolate the original model. I dunno why.\n",
    "# model_instance = wt_model.copy()\n",
    "\n",
    "# WT_250 =  parametrize_model(model_instance, 225,275,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True, fva_bounds_file='./flux_results/flux_sampling/model_bounds/7th iteration-Fixed constraint bounds 07292023/wt_250_revised_enzyme_cons.csv')\n",
    "# WT_750 = parametrize_model(model_instance, 725, 775,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True, fva_bounds_file='./flux_results/flux_sampling/model_bounds/7th iteration-Fixed constraint bounds 07292023/wt_750_revised_enzyme_cons.csv')\n",
    "# WT_1500 = parametrize_model(model_instance, 1475, 1525,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True, fva_bounds_file='./flux_results/flux_sampling/model_bounds/7th iteration-Fixed constraint bounds 07292023/wt_1500_revised_enzyme_cons.csv')\n",
    "\n",
    "# TR_250 = parametrize_model(model_instance, 225, 275,co2=22.2, if_trans=True, frac_optimum=0.90, loopless=True, pruning=True, fva_bounds_file='./flux_results/flux_sampling/model_bounds/7th iteration-Fixed constraint bounds 07292023/tr_250_revised_enzyme_cons.csv')\n",
    "# TR_750 = parametrize_model(model_instance, 725, 775,co2=22.2, if_trans=True, frac_optimum=0.90, loopless=True, pruning=True, fva_bounds_file='./flux_results/flux_sampling/model_bounds/7th iteration-Fixed constraint bounds 07292023/tr_750_revised_enzyme_cons.csv')\n",
    "# TR_1500 = parametrize_model(model_instance, 1475, 1525,co2=22.2, if_trans=True, frac_optimum=0.90, loopless=True, pruning=True, fva_bounds_file='./flux_results/flux_sampling/model_bounds/7th iteration-Fixed constraint bounds 07292023/tr_1500_revised_enzyme_cons.csv')\n",
    "\n",
    "# #Generate list of models for flux sampling\n",
    "# sampling_list = [WT_250, WT_750, WT_1500, TR_250, TR_750, TR_1500]\n",
    "# names = ['WT_250', 'WT_750','WT_1500','TR_250','TR_750','TR_1500']\n",
    "\n",
    "\n",
    "# #First save model bounds so we can reuse it again\n",
    "# for i in range(len(sampling_list)):\n",
    "#     model = sampling_list[i]\n",
    "#     model_name = names[i]\n",
    "#     name_for_file = str('flux_sample_'+model_name+'_updated_model_cons.csv')\n",
    "#     #Output the flux bounds of the model to ./fva_bounds/name_for_file\n",
    "#     save_model_bounds(model, filename=name_for_file)\n",
    "#     print(f'bounds file saved to {name_for_file}')\n",
    "    \n",
    "    \n",
    "\n",
    "# print('generating flux samples for parametrized models')\n",
    "# for i in range(len(sampling_list)):\n",
    "#     model = sampling_list[i]\n",
    "#     model_name = names[i]\n",
    "#     name_for_file = str('flux_sample_'+model_name+'_updated_model_cons.csv')\n",
    "#     #Parametrize OPTGP sampler\n",
    "#     num_samples =12000\n",
    "#     thinning = 100000\n",
    "    \n",
    "#     print('Starting OPTGP sampler:')\n",
    "#     print('num_samples: ', num_samples)\n",
    "#     print('Thinning coefficient: ', thinning)\n",
    "#     generate_flux_samples(model, num_samples =num_samples, batch_size=6000, thinning=thinning, output_filename=name_for_file)\n",
    "#     del model #Deletes model instance to free up memory \n",
    "#     print('end time: ', datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8daef58-ac51-4365-82c0-0fd130204b80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### #JULY 28 2023\n",
    "#Rerunning the sampler script due to an issue with Reaction encoding in the original model.\n",
    "\n",
    "\n",
    "#JUly 29, 2023.\n",
    "#Rerunning the script since for some reason hindi nassave yung bounds dun sa actual variable file when I used a model instance. Stupid ass \n",
    "\n",
    "\n",
    "#July 29, 2023\n",
    "#Rerunning the script doesn't work and generates infeasible solutions with FVA. I'll try to do the previous pipeline that I've done.\n",
    "#I'll rerun it again to check what happens during pruning. For some reason some crucial reactions have been pruned from the model during this time.\n",
    "\n",
    "#Try parametrizing the model to 0.80 frac optimum instead, maybe it will work\n",
    "\n",
    "\n",
    "# Generate a new log file name with current date and time\n",
    "log_file = f\"./logs/sampling_log_{get_current_datetime()}.txt\"\n",
    "sys.stdout = Logger(log_file)\n",
    "\n",
    "#Parametrize models per type by generating model instance\n",
    "\n",
    "print('Generating parametrized models for flux sampling')\n",
    "wt_model = cobra.io.read_sbml_model(\"./model/ios2164_2cell.xml\")\n",
    "\n",
    "with wt_model as model_instance:\n",
    "\n",
    "    WT_250 =  parametrize_model(model_instance, 225,275,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True,intermediate_fva_results=f'WT_250_debugging_{get_current_datetime()}', save_final_bounds_file='wt_250')\n",
    "    WT_750 = parametrize_model(model_instance, 725, 775,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True, intermediate_fva_results=f'WT_750_debugging_{get_current_datetime()}', save_final_bounds_file='wt_750')\n",
    "    WT_1500 = parametrize_model(model_instance, 1475, 1525,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True, intermediate_fva_results=f'WT_1500_debugging_{get_current_datetime()}', save_final_bounds_file='wt_1500')\n",
    "\n",
    "    TR_250 = parametrize_model(model_instance, 225, 275,co2=22.2, if_trans=True, frac_optimum=0.90, loopless=True, pruning=True, intermediate_fva_results=f'TR_250_debugging_{get_current_datetime()}', save_final_bounds_file='tr_250')\n",
    "    TR_750 = parametrize_model(model_instance, 725, 775,co2=22.2, if_trans=True, frac_optimum=0.90, loopless=True, pruning=True, intermediate_fva_results=f'TR_750_debugging_{get_current_datetime()}', save_final_bounds_file='tr_750')\n",
    "    TR_1500 = parametrize_model(model_instance, 1475, 1525,co2=22.2, if_trans=True, frac_optimum=0.90, loopless=True, pruning=True,intermediate_fva_results=f'TR_1500_debugging_{get_current_datetime()}', save_final_bounds_file='tr_1500')\n",
    "\n",
    "#Generate list of models for flux sampling\n",
    "sampling_list = [WT_250, WT_750, WT_1500, TR_250, TR_750, TR_1500]\n",
    "names = ['WT_250', 'WT_750','WT_1500','TR_250','TR_750','TR_1500']\n",
    "\n",
    "\n",
    "#First save model bounds so we can reuse it again\n",
    "for i in range(len(sampling_list)):\n",
    "    model = sampling_list[i]\n",
    "    model_name = names[i]\n",
    "    name_for_file = str('flux_sample_'+model_name+'_updated_model_cons.csv')\n",
    "    #Output the flux bounds of the model to ./fva_bounds/name_for_file\n",
    "    save_model_bounds(model, filename=name_for_file)\n",
    "    print(f'bounds file saved to {name_for_file}')\n",
    "    \n",
    "    \n",
    "\n",
    "print('generating flux samples for parametrized models')\n",
    "for i in range(len(sampling_list)):\n",
    "    model = sampling_list[i]\n",
    "    model_name = names[i]\n",
    "    name_for_file = str('flux_sample_'+model_name+'_updated_model_cons.csv')\n",
    "    #Parametrize OPTGP sampler\n",
    "    num_samples =12000\n",
    "    thinning = 100000\n",
    "    \n",
    "    print('Starting OPTGP sampler:')\n",
    "    print('num_samples: ', num_samples)\n",
    "    print('Thinning coefficient: ', thinning)\n",
    "    generate_flux_samples(model, num_samples =num_samples, batch_size=6000, thinning=thinning,processes=6, output_filename=name_for_file)\n",
    "    del model #Deletes model instance to free up memory \n",
    "    print('end time: ', datetime.datetime.now())\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e9308-185e-467c-a24d-90dc77828b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating parametrized models for flux sampling\n",
      "Read LP format model from file /tmp/tmpfud2_qou.lp\n",
      "Reading time = 0.02 seconds\n",
      ": 3956 rows, 9880 columns, 42914 nonzeros\n",
      "start time for parametrize_model():  2023-07-30 05:28:33.528844\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'trans_PPDKs_M'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     10\u001b[0m wt_model \u001b[38;5;241m=\u001b[39m cobra\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mread_sbml_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./model/ios2164_2cell.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wt_model \u001b[38;5;28;01mas\u001b[39;00m model_instance:\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     WT_250 =  parametrize_model(model_instance, 225,275,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True,intermediate_fva_results=f'WT_250_debugging_{get_current_datetime()}', save_final_bounds_file='wt_250')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     TR_250 = parametrize_model(model_instance, 225, 275,co2=22.2, if_trans=True, frac_optimum=0.90, loopless=True, pruning=True, intermediate_fva_results=f'TR_250_debugging_{get_current_datetime()}', save_final_bounds_file='tr_250')\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     TR_750 \u001b[38;5;241m=\u001b[39m \u001b[43mparametrize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m725\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m775\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mco2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m22.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_trans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrac_optimum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.90\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloopless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpruning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_fva_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTR_750_debugging_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mget_current_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_final_bounds_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtr_750\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     TR_1500 \u001b[38;5;241m=\u001b[39m parametrize_model(model_instance, \u001b[38;5;241m1475\u001b[39m, \u001b[38;5;241m1525\u001b[39m,co2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m22.2\u001b[39m, if_trans\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, frac_optimum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.90\u001b[39m, loopless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, pruning\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,intermediate_fva_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTR_1500_debugging_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_current_datetime()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, save_final_bounds_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr_1500\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#Generate list of models for flux sampling\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 56\u001b[0m, in \u001b[0;36mparametrize_model\u001b[0;34m(model, ppfd_low, ppfd_high, co2, if_trans, loopless, frac_optimum, pruning, fva_bounds_file, intermediate_fva_results, save_final_bounds_file)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m if_trans\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     add_trans_reactions(model_instance)\n\u001b[0;32m---> 56\u001b[0m     \u001b[43madd_trans_constraints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_instance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#Generate list of essential reactions in the model that must not be turnned off\u001b[39;00m\n\u001b[1;32m     60\u001b[0m essential_reactions \u001b[38;5;241m=\u001b[39m cobra\u001b[38;5;241m.\u001b[39mflux_analysis\u001b[38;5;241m.\u001b[39mfind_essential_reactions(model_instance, processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 440\u001b[0m, in \u001b[0;36madd_trans_constraints\u001b[0;34m(model, trans_pepc_rates, trans_ppdks_rates, trans_mdh_rates, trans_nadp_me_rates, trans_CA_rates)\u001b[0m\n\u001b[1;32m    437\u001b[0m model\u001b[38;5;241m.\u001b[39madd_cons_vars(trans_PEPC_cons)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m#PPDK constraint\u001b[39;00m\n\u001b[0;32m--> 440\u001b[0m trans_PPDKs_M  \u001b[38;5;241m=\u001b[39m \u001b[43mmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_rxn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrans_PPDKs_M\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m trans_PPDKs_BS \u001b[38;5;241m=\u001b[39m mm\u001b[38;5;241m.\u001b[39mget_rxn(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrans_PPDKs_BS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    442\u001b[0m wt_PPDKs_M \u001b[38;5;241m=\u001b[39m mm\u001b[38;5;241m.\u001b[39mget_rxn(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPPDKs_M\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/git_repos/RiceMM/./src/model_manipulation.py:65\u001b[0m, in \u001b[0;36mget_rxn\u001b[0;34m(model, reaction_name)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_rxn\u001b[39m(model, reaction_name):\n\u001b[1;32m     64\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[0;32m---> 65\u001b[0m \t\t\u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreactions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_by_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreaction_name\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     66\u001b[0m \t\t\t\u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mreactions\u001b[38;5;241m.\u001b[39mget_by_id(reaction_name)\n\u001b[1;32m     67\u001b[0m \t\t\u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/RiceMM/lib/python3.10/site-packages/cobra/core/dictlist.py:73\u001b[0m, in \u001b[0;36mDictList.get_by_id\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_by_id\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mid\u001b[39m: Union[Object, \u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Object:\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the element with a matching id.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'trans_PPDKs_M'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating parametrized models for flux sampling\n",
      "Read LP format model from file /tmp/tmp_6l9t3os.lp\n",
      "Reading time = 0.02 seconds\n",
      ": 3956 rows, 9880 columns, 42914 nonzeros\n",
      "start time for parametrize_model():  2023-07-30 05:31:36.423555\n",
      "reading previous FVA bounds\n",
      "Reactions to remove: 2912\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Generate a new log file name with current date and time\n",
    "log_file = f\"./logs/sampling_log_{get_current_datetime()}.txt\"\n",
    "sys.stdout = Logger(log_file)\n",
    "\n",
    "#Parametrize models per type by generating model instance\n",
    "\n",
    "print('Generating parametrized models for flux sampling')\n",
    "wt_model = cobra.io.read_sbml_model(\"./model/ios2164_2cell.xml\")\n",
    "\n",
    "with wt_model as model_instance:\n",
    "\n",
    "#     WT_250 =  parametrize_model(model_instance, 225,275,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True,intermediate_fva_results=f'WT_250_debugging_{get_current_datetime()}', save_final_bounds_file='wt_250')\n",
    "#     WT_750 = parametrize_model(model_instance, 725, 775,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True, intermediate_fva_results=f'WT_750_debugging_{get_current_datetime()}', save_final_bounds_file='wt_750')\n",
    "#     WT_1500 = parametrize_model(model_instance, 1475, 1525,co2=29, if_trans=False, frac_optimum=0.90, loopless=True, pruning=True, intermediate_fva_results=f'WT_1500_debugging_{get_current_datetime()}', save_final_bounds_file='wt_1500')\n",
    "\n",
    "#     TR_250 = parametrize_model(model_instance, 225, 275,co2=22.2, if_trans=True, frac_optimum=0.90, loopless=True, pruning=True, intermediate_fva_results=f'TR_250_debugging_{get_current_datetime()}', save_final_bounds_file='tr_250')\n",
    "    TR_750 = parametrize_model(model_instance, 725, 775,co2=22.2, if_trans=True, frac_optimum=0.90, loopless=True, pruning=True, intermediate_fva_results=f'TR_750_debugging_{get_current_datetime()}', save_final_bounds_file='tr_750')\n",
    "    TR_1500 = parametrize_model(model_instance, 1475, 1525,co2=22.2, if_trans=True, frac_optimum=0.90, loopless=True, pruning=True,intermediate_fva_results=f'TR_1500_debugging_{get_current_datetime()}', save_final_bounds_file='tr_1500')\n",
    "\n",
    "#Generate list of models for flux sampling\n",
    "sampling_list = [WT_250, WT_750, WT_1500, TR_250, TR_750, TR_1500]\n",
    "names = ['WT_250', 'WT_750','WT_1500','TR_250','TR_750','TR_1500']\n",
    "\n",
    "\n",
    "#First save model bounds so we can reuse it again\n",
    "for i in range(len(sampling_list)):\n",
    "    model = sampling_list[i]\n",
    "    model_name = names[i]\n",
    "    name_for_file = str('flux_sample_'+model_name+'_updated_model_cons.csv')\n",
    "    #Output the flux bounds of the model to ./fva_bounds/name_for_file\n",
    "    save_model_bounds(model, filename=name_for_file)\n",
    "    print(f'bounds file saved to {name_for_file}')\n",
    "    \n",
    "    \n",
    "\n",
    "print('generating flux samples for parametrized models')\n",
    "for i in range(len(sampling_list)):\n",
    "    model = sampling_list[i]\n",
    "    model_name = names[i]\n",
    "    name_for_file = str('flux_sample_'+model_name+'_updated_model_cons.csv')\n",
    "    #Parametrize OPTGP sampler\n",
    "    num_samples =12000\n",
    "    thinning = 100000\n",
    "    \n",
    "    print('Starting OPTGP sampler:')\n",
    "    print('num_samples: ', num_samples)\n",
    "    print('Thinning coefficient: ', thinning)\n",
    "    generate_flux_samples(model, num_samples =num_samples, batch_size=6000, thinning=thinning,processes=6, output_filename=name_for_file)\n",
    "    del model #Deletes model instance to free up memory \n",
    "    print('end time: ', datetime.datetime.now())\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14514c16-6e44-4b85-b092-be0439cfc79e",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- Flux bounds for FVA show infeasibility for Trans parametrized models. Maybe we could instead prune to model beforehand before computing loopless FVA?\n",
    "- Other constraints \n",
    "\n",
    "-One of the Photon decomposition reactions cause infeasibility issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fcbe37-4dad-497b-95cc-0d3fa27a326e",
   "metadata": {},
   "source": [
    "The following scripts are nott used but were used for prototyping as I was looking for a way to implement Loopless-FVA without destroying the sample distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db0e55-545d-4198-b5b6-e2e6ba7a02d4",
   "metadata": {},
   "source": [
    "The second part of this script is for post-processing the DFs obtained via flux sampling using the CycleFreeFlux algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39b10e5-c885-4c13-81c9-911f892789cc",
   "metadata": {},
   "source": [
    "Notes: needed to repeat sampling due to the following:\n",
    "- sampling matrices are too large to load, compensate with the use of thinning instead. Use specifications from Hermann et al (2020) which uses instead a thinning coefficient of 10000 instead of keeping all samples. My original parameters were n = 50000, batch size = 2000 and thinning = 500. \n",
    "- Maybe I should use a thinning coefficient of 100000 instead? Since my model has a number of dimensions one exponent higher than the previously benchmarked Arnold Model\n",
    "    - I've decided to run my model with a thinning coefficient of 25000. Hopefully autocorrelation and convergence wouldn't be much of an issue. Total samples would be equal in turn to 1.25e8 individual sampling points. \n",
    "\n",
    "- I needed to reparametrize samples considering that the objective function doesn't apply in flux sampling. INstead of adding an objective coefficient all demand reactions that output biomass are turned off except for \"DM_Phloem_BS\". What I need to do instead is re-parametrize it to allow photoassimilates to exit at that reaction rather than to others.\n",
    "- I'll save the format to \".npz\" instead of \"csv\" since I'm running out of memory whenever I'm loading it out of this script. I think the output of the model is sparse enough that I can instead use this format instead.\n",
    "    - Actually I can save my csv to a normal csv once since I can just adjust the thinning number instead of keeping all samples.\n",
    "\n",
    "- It is still a question whether my samples are uncorrelated or not. I will run the diagnostic tests after I finish running the rest of the scripts.\n",
    "\n",
    "\n",
    "\n",
    "Note: May 17 2023\n",
    "Flux sampling script was interrupted @ TR750 and was rerun at that point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae799577-6b2a-444f-9f90-aa72715e0ec2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Notes: \n",
    "OPTGP Is shown to be faster than ACHR and also converges faster\n",
    "\n",
    "Let's try with a 10 samples first with 10 batches\n",
    "\n",
    "I think the model is too large to be loaded into the flux sampler. \n",
    "\n",
    "It initialized after 30 minutes, let's try sampling na. \n",
    "\n",
    "It takes 1-2 hours at most to generate 2000 samples with a thinning coefficient of 10000. Extrapolating from that, we need probably 4-6 hours to generate 5000 samples with a TC of 20000. To keep things tractable I'll keep the thinning coefficient at 10000. I can just add more samples if needed.\n",
    "\n",
    "\n",
    "Things to add to the paper:\n",
    "\n",
    "\n",
    "Flux sampling is another constraints-based technique used for characterizing the null space of a given stoichiometric model, providing us insights on the flux distributions of a given metabolic model without the explicit definition of a given cellular objective. Not only does this technique offer advantages over conventional FBA and FVA where it allows researchers to determine the feasibility of solutions within a given defined range, as well as the distribution of these fluxes provided the model constaints.. The latter, in turn, allows statistical analysis to determine whether any given fluxes exist within a single probability distribution or not. This, in turn, allows a more direct comparison of fluxes and to adequately sample a solution space provided the constraints implemented in the model.\n",
    "\n",
    "The same constraints have been implemented as with the previous set-ups but with a modification with regards to how the objective functions are concerned. Unlike in pFBA where we can set a particular reaction as an objective, no such setting exists for flux sampling. Thus, this setup is in turn reliant on explicitly defined constraints that would define the n reaction-dimensional solution space. To parametrize this, we have turned off all biomass-related demands such as Coleoptile Biomass, Straw_Biomass and the \"DM_Phloem_M\" reaction to force the model to output flux to the \"DM_Phloem_BS\" reaction. Additionally, the model is constrained in a similar manner as with the previous pFBA setups that we had done to benchmark and assess our model's performance.\n",
    "\n",
    "THe benefits of using Flux Sampling compared to other methods such as FVA in characterizing a solution space is that:\n",
    "a. We may establish confidence intervals with the use of statistical tests to fully characterize a given solution space and whether\n",
    "b. Rather than simply generating representative fluxes showing the maximum and minimum amount of possible flux towards a particular reaction, flux sampling allows summary statistics to be generated, including the probability distribution of each particular reaction. \n",
    "c. Lastly, flux sampling allows a detailed comparison of the interdependence of each particular reaction, showing which reactions are coupled to each other in a positive and negative fashion. This is done\n",
    "\n",
    "Three setups per parametrized model were initialized, each with a representative light treatment of 250, 750 and 1500 PPFD with a defined range of +-25, respectively. The NGAM reactions were instead set at a static value at each representative light point detailed above. A total of 6 treatments were initialized to represent both the WT and Trans models at the selected light points. \n",
    "\n",
    "The sampling algorithm used was the OPTGP (optimized general parallel) sampler from the Cobrapy.sampling package. The sampler was parametrized to generate 5000 samples with a thinning coefficient of 25000, representing 1.25x10^8 sampling points, and was multithreaded into 7 processes each. Run times varied from between 3-6 hours per model parametrized, which was expected based on the number of reactions in the parametrized model.\n",
    "\n",
    "\n",
    "After the generation of individual sampling points, convergence statistics were done to assess the level of autocorrelation as well as to assess whether the samplers have sufficiently converged to a singular value. The former is done with the use of the \"acf\" function to assess the level of autocorrelation on each given column, while the latter is assessed with the use of two specific methods -- the Gelman-Rubin and the Geweke statistics. These are two methods that we will use to assess whether the number of sampling points is sufficient to ensure adequate convergence of each reaction in the model. Reactions that have not converged sufficiently will be indicated.\n",
    "\n",
    "After the assessment of autocorrelation and convergence, each of the pairs between light treatments (225-725, 725-1475, 225-1475) and between models (WT and Trans) are subjected to pairwise Kruskal-Wallis tests to assess which reaction pairs are derived from the same distribution. It is a non-parametric rank-based test test suitable for comparing outputs where there are no assumed distributions behind a population, and has been used in a similar fashion in previous flux sampling setups that involve Plant Metabolism  (Herrmann et al., 2019).  \n",
    "\n",
    "This procedure, in turn, allows us to deduce in finer detail which reactions in particular are positively correlated with CO2 flux into the BS cell's Rubisco reaction. In the previous pFBA experiment we were able to demonstrate a similar approach to this by performing a sensitivity analysis on Glycine Decarboxylase, and had demonstrated the negative relationship between M cell GLYDHD and BS Cell GLYDHD.\n",
    "\n",
    "Lastly, we can afterwards assess which fluxes are coupled by iterating over the list of fluxes in a single flux sample matrix and computing the spearman correlations between the pairwise comparisons. From here we can assess which particular reactions are positively or negatively correlated or not. A particular focus will then be held. Further Downstream analysis includes determination of reactions that are positively or negatively coupled with each other and corroborating this fact on whether the reaction is disrupted in the \"Trans\" parametrized model.  \n",
    "\n",
    "\n",
    "\n",
    "May 19, 2023\n",
    "Some observations on the fluxes obtained from initial flux sampling runs:\n",
    "- It seems it doesn't maximize CO2 assimilation as with pFBA. \n",
    "- It features some reactions with infeasibly large fluxes, particularly those expected to have flux cycling.\n",
    "\n",
    "I asked the Gitter group on what are their thoughts on how to approach this.\n",
    "\n",
    "One solution I think is this:\n",
    "- Reparametrize a model first and generate FVA solutions to \"pre-process\" the model, and in turn add directionality and bound constraints to the model to reduce its solution space further.\n",
    "- Try readding the objective function for photoassimilate generation as well as the pfba objective, as well as add the \"cyclefreeflux\" function by Desouki et al (2015).\n",
    "\n",
    "\n",
    "\n",
    "I am currently testing the Latter.\n",
    "\n",
    "May 20, 2023\n",
    "\n",
    "The latter does not work since it applies MILP, I think. It returns the following error:\n",
    "    \n",
    "    TypeError: Sampling does not work with integer problems.\n",
    "\n",
    "I will instead to the former instead. I've also implemented pruning to remove any unused reactions and metabolites based on the find_blocked_reactions() function of FVA. This will further constrain the model to sort of \"Contextualise\" it. \n",
    "\n",
    "\n",
    "May 20, 2023\n",
    "\n",
    "Based on the Geweke Statistic, and with a z-score of 1.96 (indicating 0.95 confidence  interval) most of the samples have converged on a single statistic. Based on the Gelman Rubin statistic however all of the reactions have not converged to a singular solution. Why?\n",
    "\n",
    "I've rerun the diagnostic scripts in R using Coda and have produced a more reliable measure of convergence and autocorrelation. In both cases only \n",
    "\n",
    "I'll test 5000x25000 samples with the reduced and pFVA-constrained models. I think sampling will be faster considering that I've pruned the samples as well as reduced the solution space by a lot. \n",
    "\n",
    "\n",
    "Flux sampling convergence statistics are based on the methods highlighted by Hermmann et al (2019) and by Fallahi et al (2020). It says that we shouldn't use the normal Gelman-Rubin statistic and instead use the Brooks-Gelman formulation instead.\n",
    "\n",
    "However, for both cases I've instead used the Raftery-Lewis statistic and \n",
    "the Geweke Diagnostic to assess convergences for all parameters. I decided to re-run instead the two last samples considering that they both have significant amounts of reactions that haven't converged based on the RL statistic and the GW statistic.\n",
    "\n",
    "For the RL statistic both 250 and 750 parametrized samplers have converged, although the Geweke diagnostic only reports a convergence rate of around 70 percent. In 1500 the convergence rate falls to 40 percent only which necessitates the re-run scenario. AFterwards I can simply re-run the scripts and re-assess my results. In the meantime however I can analyze both 250 and 750 scenarios as well the highb light scenarios particularly those with significantly varying distributions\n",
    "\n",
    "\n",
    "June 13, 2023\n",
    "\n",
    "I have rerun both WT andTR 1500 to validate if both have converged. If it hasn't I'll report the results as-is and include it in the discussion.\n",
    "\n",
    "June 15, 2023\n",
    "\n",
    "I have an idea in order to ensure convergence as well as to reduce runtimes.\n",
    "My idea is to remove all reactions that are not foundd in the model to 0 in order to reduce the nullspace of the model. This will be based on the initial runs for the flux sampling runs. \n",
    "\n",
    "Afterwards I need to compare their distributions based dun sa mga previous runs to determine if pareho yung distribution. If it is the same or virtually the samme I will use it because I can ensure that it has a higher convergence rate than the one with lower samples.\n",
    "\n",
    "If in case this works then it should return the same distribution and my samples would've converged faster. Furthermore there isn't need to go for breadth considering that my parametrizations are already fixed. Hopefully it can converge faster but since I've changed the thinning constant to 100k it may change the distribution.\n",
    "\n",
    "June 15 2023\n",
    "\n",
    "    Filtering and Increasing the thinning doesn't work apparently. It reduced the convergence rate of my reactions by almost 20 percent -- very alarming. What I should do instead is to increase the thinning coefficient to something more ok (around 35000) while still being relatively fast enough.\n",
    "\n",
    "    However I'll be re-running my sampling attempt in order to fix some errors with H2O cycling. (Change H2O flux to be unidirectional towards M cell to reflect transpiration mechanism. Before what happened was that )\n",
    "\n",
    "    Furthermore I fixed the flux bounds for Photon flux. Apparently I had a typo dun sa upper bounds nung TR_1500 which caused it to have 25 lesser flux than normal.\n",
    "\n",
    "    Hopefully at the end of this ok na siya. Wala na kong problem after nito -- analysis na lang.\n",
    "\n",
    "\n",
    "June 17, 2023\n",
    "\n",
    "I just discovered how inflated the flux values are for high light conditions, which indicate that there is significant looping in some of my solutions, which I need to reassess considering how central they are to my model, particularly yung reactions involving Malate.\n",
    "\n",
    "I can try to add Loopless FVA then compare the results with the normal runs. Kahit paspasan lang.\n",
    "\n",
    "\n",
    "# #Notes: June 17\n",
    "# #Adding the \"Loopless FVA\" parameter to the pre-processing step does not work and generates an infeasible solution. It causes the solver to become stuck.\n",
    "\n",
    "# #Note: it takes 30 minutes to implement Loopless FVA to the model. Upon checking yung mga flux bounds it reduces the max and min values to something closer to their pFBA counterparts. \n",
    "# #I'll try to rerun my pipeline to accomodate that.\n",
    "# #Note: Loopless FVA produces NaNs in the flux bounds of some reactions. Need to remove that.\n",
    "\n",
    "\n",
    "June 19, 2023:\n",
    "\n",
    "I think I've finally optimized my Sampling runs. I just need to rerun my scripts kahit 25000 thinning lang since increasing it didn't really affect yung convergence rates.\n",
    "\n",
    "\n",
    "Things I have tested (So far)\n",
    "- Filtering out reactions before sampling -- Convergence rates lower almost 20 percent accorss the board.\n",
    "- 10000 Thinning\n",
    "- 25000 Thinning\n",
    "- 35000 Thinning -- little improvement in convergence rates, in fact it lessened percent converged \n",
    "- Using add_loopless() function -- doesn't work, turns the model to an MILP problem which breaks Sampling\n",
    "- Loopless FVA to define reaction bounds == works so far but it breaks during actual sampling (cannot generate warmup points due to overconstrained model).\n",
    "- Using _add_cyclefree_flux() -- doesn't work, breaks the model and causes numerical instability.\n",
    "- LOopless FVA with \"relaxed option\" for NaNs --- I think this method works. Wala na kong nakikitang loops. However this method runs really slow (~2 hours)\n",
    "\n",
    "June 20, 2023\n",
    "- I have problems regarding sampling right now. Sigh. I don't even know the issue behind it rn it just breaks\n",
    "- I know now -- apparently using the loopless option overconstrains the model and prevents it from generating warmup points using the sampler. I didn't see that it works. Maybe it'll work if I modify the parametrization to a fraction of the optima instead?\n",
    "    - APparently kasi it c\n",
    "\n",
    "- What I can do is probably to post-process my flux samples using the \"loopless_solution\" method highlighted in Desouki et al's paper.\n",
    "    - This method works. I checked the values for CSm_M and it does deflate the values significantly.\n",
    "    - It looks like it might work -- however it is fairly slow when run in series. I'll try to run them in parallel\n",
    "        - Multiprocess now works but it keeps doing i/o operations on the models. Can it be modified to just do a single I/O operation?\n",
    "            It becomes stuck if I try it\n",
    "            \n",
    "                It's such a hassle to use multiprocess. I'll just run it on a single thread tutal mabilis lang naman. I'm wasting time on how to run it on a multithread e kaya naman ng isang thread.\n",
    "\n",
    "\n",
    "            June 22, 2023\n",
    "\n",
    "            - CycleFreeFlux destroys the uniform distribution of my flux solutions, preventing me from inferring any     information based dun sa distributions nung fluxes. Therefore this should be rewritten. \n",
    "\n",
    "            I've found a methodology similar to Hay et al. (2014) which makes use of loopless FVA to set the bounds for the model. I've implemented this but I've used CFF to generate flux ranges given a set of solutions. However, unlike Hay et al I am ablle to use CFF globally instead of restricting it to certain loop reactions only. \n",
    "\n",
    "June 24, 2023\n",
    "\n",
    "I've revised the script so that it properly bypasses infeasibility issues now. Apparently the previous iteration didn't exactly resolve it. However, I've already computed the samples for WT_250 so I can skip that now -- it didn't have too many feasibility issues so I could skip over that.\n",
    "\n",
    "June 28, 2023\n",
    "\n",
    "Need to rerun WT_750 and WT1500 due to numerical isseus. I'm not sure why it wasn't caught by the script but I have modified it \n",
    "\n",
    "\n",
    "July 28, 2023\n",
    "\n",
    "Apparently I've caught an error after reviewing the constraints portion of my model Mali encoding ko of the following:\n",
    "-NADP-ME reactions\n",
    "-MDH-related reactions\n",
    "\n",
    "Instead of cosntraining the MDH reactions properly it seems I have constrained it to one direction only, which may explain why no malate is being produced in the model. I've encoded this very early on so di ko napansin. It's a really stupid mistake.\n",
    "\n",
    "I've encountered some numerical issues that had been fixed simply by increasinng the model tolerance to 1e-9. I think also putting my model as a contextual instance also helped -- I suspect some form of \"Batch contamination\" being carried over from previous iterations causing the model to be unsolvable once it encounters an infeasible solution.\n",
    "\n",
    "July 28, 2023\n",
    "\n",
    "Fixed an issue with Nproj. Default value should be None, not 0 otherwise it causes zero division error.\n",
    "ALso what I've noticed is that increasing the tolerance to 1e-9 solves a lot of numerical issues in the model. \n",
    "\n",
    "\n",
    "I've fixed the model accordingly. Same parameters lang and same sampling methodology but I'll resample the model fully with FVA bounds para ok.\n",
    "\n",
    "Notes:\n",
    "\n",
    "It seems hindi naayos yung bounds kapag model instance ginamit. I think I just need to use yung model.copy() function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b412192-58d7-4665-b013-58b6a313e2fa",
   "metadata": {},
   "source": [
    "METHODOLOGY\n",
    "\n",
    "1.\n",
    "Run convergence statistics on each and generate plots to assess total convergence stats for each CSV. These will include tests such as the \n",
    "Raftery-Lewis statistic and the Geweke statistics to assess both autocorrelation as well as assess convergence.\n",
    "\n",
    "Afterwards get only the flux names of those reactions that have converged\n",
    "Run pairwise Kruskal-wallis tests per CSV using the above list of converged reactions\n",
    "Identify each reaction with significant and non-significant distributions each\n",
    "\n",
    "\n",
    "Generate histograms/probability densities for relevant reactions with significantly different distributions with WT and Trans models.\n",
    "2. Flux coupling analysis\n",
    "Check which fluxes are coupled with each otehr and identify which fluxes are then related to each other, particularly Carbon Fixation reactions in the BS cell such as Rubisco and the DM_Phloem reactions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
