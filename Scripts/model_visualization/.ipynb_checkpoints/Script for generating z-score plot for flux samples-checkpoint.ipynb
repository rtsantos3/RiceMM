{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a656bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T11:08:41.205406Z",
     "start_time": "2023-03-23T11:08:39.074105Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-05-09\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os\n",
    "import cobra\n",
    "import cplex \n",
    "import libsbml\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import memote\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xlsxwriter\n",
    "import path \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from itertools import combinations\n",
    "from scipy.stats import kruskal, mannwhitneyu\n",
    "import multiprocessing\n",
    "from itertools import combinations, product\n",
    "from multiprocessing import Pool\n",
    "from sklearn.decomposition import PCA\n",
    "import mplcursors\n",
    "\n",
    "\n",
    "from sklearn.linear_model  import LinearRegression\n",
    "\n",
    "#Change working dir first, ty ChatGPT, much loves\n",
    "cwd = os.getcwd()\n",
    "# Split the path into a list of directories\n",
    "directories = cwd.split(os.sep)\n",
    "# Remove the last two directories from the list\n",
    "directories = directories[:-2]\n",
    "# Join the directories back into a path\n",
    "new_cwd = os.sep.join(directories)\n",
    "# Change the current working directory to the new path\n",
    "os.chdir(new_cwd)\n",
    "\n",
    "sys.path.append(\"./src/\")\n",
    "\n",
    "import model_initialize as mi\n",
    "import model_manipulation as mm\n",
    "\n",
    "\n",
    "#Set solver to gurobi\n",
    "config = cobra.Configuration()\n",
    "config.solver = 'gurobi'\n",
    "\n",
    "#Read 2-cell model\n",
    "wt_model = cobra.io.read_sbml_model(\"./model/ios2164_2cell.xml\")\n",
    "trans_model = cobra.io.read_sbml_model(\"./model/ios2164_2cell.xml\")\n",
    "#Estimate inf\n",
    "inf = 1e6\n",
    "\n",
    "\n",
    "#Add trans reactions to trans_model\n",
    "mi.add_trans_reactions(trans_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b7ca3-4c3d-4250-98f8-d84c422a5a39",
   "metadata": {
    "tags": []
   },
   "source": [
    "Pipeline breakdown:\n",
    "\n",
    "Load CSVs to a memory saving format first\n",
    "\n",
    "1.\n",
    "Run convergence statistics on each and generate plots to assess total convergence stats for each CSV. These will include tests such as the Geweke statistic. \n",
    "Afterwards get only the flux names of those reactions that have converged\n",
    "Run pairwise Kruskal-wallis tests per CSV using the above list of converged reactions\n",
    "Identify each reaction with significant and non-significant distributions each\n",
    "\n",
    "Generate histograms/probability densities for relevant reactions with significantly different distributions with WT and Trans models\n",
    "\n",
    "2. Flux coupling analysis\n",
    "Check which fluxes are coupled with each otehr and identify which fluxes are then related to each other, particularly Carbon Fixation reactions in the BS cell such as Rubisco and the DM_Phloem reactions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "405afc4c-5980-4e21-b14f-e3045ac097e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''This code block contains functions that we need to use to analyze our flux sampling experiment. \n",
    "These include tests to check for convergence, for autocorrelation, as well as for pairwise comparisons of fluxes between\n",
    "parametization regimens (WT, TR, as well as 3 light conditions) and a script to determine whether fluxes are\n",
    "coupled with each other. Lastly, I also tried PCA to determine which reactions contribute most to the variance of each null space distribution per\n",
    "sampling run.\n",
    "'''\n",
    "\n",
    "def get_df_name(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    return name\n",
    "\n",
    "\n",
    "def load_csv_and_cleanup(filename, tol=1e-7):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Remove columns with a mean below tolerance\n",
    "    mean_values = df.mean()\n",
    "    columns_to_remove = mean_values[abs(mean_values) < tol].index\n",
    "    df = df.drop(columns=columns_to_remove)\n",
    "\n",
    "    # Fix the indices\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(df.shape)\n",
    "\n",
    "    return df    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def compare_pair(reaction, df1, df2, significance_threshold):\n",
    "    \"\"\"\n",
    "    Compare the flux distributions of a pair of columns from two dataframes using the Kruskal-Wallis test.\n",
    "\n",
    "    :param pair: Tuple containing the pair of column names.\n",
    "    :param df1: The first dataframe.\n",
    "    :param df2: The second dataframe.\n",
    "    :param significance_threshold: The significance threshold to use for the Kruskal-Wallis test.\n",
    "    :return: The pair and the result of the Kruskal-Wallis test.\n",
    "    \"\"\"\n",
    "    sample1 = df1[reaction]\n",
    "    sample2 = df2[reaction]\n",
    "    H, pval = kruskal(sample1, sample2)\n",
    "    return pval \n",
    "\n",
    "def compare_flux_distributions(df1, df2, significance_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Compare the flux distributions of each pair of columns from two dataframes using the Kruskal-Wallis test.\n",
    "\n",
    "    :param df1: The first dataframe.\n",
    "    :param df2: The second dataframe.\n",
    "    :param significance_threshold: The significance threshold to use for the Kruskal-Wallis test (default: 0.05).\n",
    "    :return: A list of pairs with significantly different distributions.\n",
    "    \"\"\"\n",
    "    common_columns = set(df1.columns) & set(df2.columns)\n",
    "    \n",
    "    pool = multiprocessing.Pool()\n",
    "    sig_results = []\n",
    "    non_sig_results = []\n",
    "    \n",
    "    for rxn in common_columns:\n",
    "        kw_pval = compare_pair(rxn, df1, df2, significance_threshold) #This does the Kruskal-Wallis part\n",
    "        if kw_pval < significance_threshold:\n",
    "            sig_results.append(rxn)\n",
    "        else:\n",
    "            non_sig_results.append(rxn)\n",
    "            \n",
    "    #Sort the outputs before returning\n",
    "    sig_results.sort()\n",
    "    non_sig_results.sort()\n",
    "            \n",
    "    \n",
    "    return sig_results, non_sig_results\n",
    "\n",
    "\n",
    "\n",
    "#The script above is used for flux coupling using the opened CSV file as input and outputs 3 lists:\n",
    "#positively correlated, neg. correlated and uncoupled reactions\n",
    "\n",
    "\n",
    "#This is for visualization\n",
    "def generate_histograms(dataframe1, dataframe2, column_name):\n",
    "    # Create subplots for two histograms\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "    # Histogram for dataframe1\n",
    "    axes[0].hist(dataframe1[column_name], bins=10, color='skyblue')\n",
    "    axes[0].set_title(f'Histogram of {column_name}')\n",
    "    axes[0].set_xlabel(column_name)\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    # Histogram for dataframe2\n",
    "    axes[1].hist(dataframe2[column_name], bins=10, color='lightgreen')\n",
    "    axes[1].set_title(f'Histogram of {column_name}')\n",
    "    axes[1].set_xlabel(column_name)\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the histograms\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def generate_stacked_histogram(dataframes, column_name,histtype='bar'):\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    # Concatenate the column data from all dataframes into a single series\n",
    "    \n",
    "    df_name = list()\n",
    "    \n",
    "    \n",
    "    for dfs in dataframes:\n",
    "    # Create the stacked histogram\n",
    "        if column_name in dfs.columns:\n",
    "\n",
    "            plt.hist(dfs[column_name], bins=33, alpha=0.45, stacked=True, density=False,histtype=histtype)\n",
    "            df_name.append(get_df_name(dfs))\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Set plot title and labels\n",
    "    plt.title(f\"Stacked Histogram for Column: {column_name}\")\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.legend(df_name)\n",
    " \n",
    "    \n",
    "    plt.figure().set_figheight(1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_scatter_regression(df, column_x, column_y):\n",
    "    \"\"\"\n",
    "    Generates a scatterplot and a linear regression line between two columns in a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        - df (pandas.DataFrame): The input dataframe.\n",
    "        - column_x (str): The column name for the X-axis.\n",
    "        - column_y (str): The column name for the Y-axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract X and Y values from the dataframe\n",
    "    X = df[column_x].values.reshape(-1, 1)\n",
    "    Y = df[column_y].values\n",
    "\n",
    "    # Fit linear regression model\n",
    "    regression = LinearRegression()\n",
    "    regression.fit(X, Y)\n",
    "\n",
    "    # Predict Y values based on the regression line\n",
    "    Y_pred = regression.predict(X)\n",
    "\n",
    "    # Plot the scatterplot and regression line\n",
    "    plt.scatter(X, Y, color='blue', label='Actual')\n",
    "    plt.plot(X, Y_pred, color='red', label='Regression Line')\n",
    "    plt.xlabel(column_x)\n",
    "    plt.ylabel(column_y)\n",
    "    plt.title('Scatterplot with Linear Regression Line')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#These functions are for generating PCA plots and other corollary analyses\n",
    "\n",
    "def merge_dataframes(dataframes, names):\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    for i, df in enumerate(dataframes):\n",
    "        # Get the corresponding name from the names list\n",
    "        df_name = names[i]\n",
    "        \n",
    "        # Add a new column with the dataframe name\n",
    "        df['sample'] = df_name\n",
    "        \n",
    "        # Merge dataframes\n",
    "        if merged_df.empty:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "    \n",
    "    # Remove NAs\n",
    "    merged_df = merged_df.fillna(1e-7)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def generate_pca_plot(dataframe):\n",
    "    numeric_columns = dataframe.select_dtypes(include=[np.number]).columns\n",
    "    X = dataframe[numeric_columns].values\n",
    "\n",
    "    pca = PCA(n_components=3)\n",
    "\n",
    "    components = pca.fit_transform(X)\n",
    "\n",
    "    # Extract dataframe names\n",
    "    dataframe_names = dataframe['sample'].unique()\n",
    "\n",
    "    # Generate plot with different colors for each dataframe\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'yellow']  # Add more colors if needed\n",
    "\n",
    "    #Generate figure and subplot\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    for i, name in enumerate(dataframe_names):\n",
    "        indices = dataframe['sample'] == name\n",
    "        ax.scatter(components[indices, 0], components[indices, 1], label=name, c=colors[i])\n",
    "\n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "    ax.set_zlabel('PC3')\n",
    "    ax.legend()\n",
    "    \n",
    "    mplcursors.cursor(hover=True)\n",
    "    plt.show()\n",
    " \n",
    "    #export PCA to determine component variance\n",
    "    \n",
    "    pca_fit = pca.fit(X)\n",
    "    \n",
    "    return pca_fit\n",
    "\n",
    "def select_significant_features(pca, feature_names, top_n):\n",
    "    loadings = pca.components_\n",
    "    abs_loadings = np.abs(loadings)\n",
    "    feature_contribution = np.sum(abs_loadings, axis=0)\n",
    "    sorted_features = feature_names[np.argsort(feature_contribution)[::-1]]\n",
    "    selected_features = sorted_features[:top_n]\n",
    "    return selected_features\n",
    "\n",
    "def plot_selected_features_heatmap(pca, feature_names, selected_features):\n",
    "    loadings = pca.components_\n",
    "    selected_indices = [np.where(feature_names == feature)[0][0] for feature in selected_features]\n",
    "    selected_loadings = loadings[:, selected_indices]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(selected_loadings, cmap='coolwarm', annot=False, xticklabels=selected_features)\n",
    "    plt.xlabel('Selected Features')\n",
    "    plt.ylabel('Principal Components')\n",
    "    plt.title('Loadings Heatmap of Selected Features')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "def visualize_correlation_matrix(df, sort_by=None, ascending=False):\n",
    "    \"\"\"\n",
    "    Calculates and visualizes the correlation matrix based on flux measurements in a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        - df (pandas.DataFrame): The input dataframe containing flux measurements.\n",
    "        - sort_by (str or None): Column name to sort the correlation matrix by. Default is None.\n",
    "        - ascending (bool): Whether to sort in ascending order. Default is False.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = df.corr()\n",
    "\n",
    "    # Sort the correlation matrix if sort_by is specified\n",
    "    if sort_by is not None:\n",
    "        correlation_matrix = correlation_matrix.sort_values(by=sort_by, ascending=ascending)\n",
    "\n",
    "    # Create a heatmap of the correlation matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True)\n",
    "\n",
    "    # Set plot title\n",
    "    plt.title('Correlation Matrix')\n",
    "\n",
    "    # Display the heatmap\n",
    "    plt.show()\n",
    "\n",
    "def filter_correlation_pairs(correlation_matrix, threshold, filter_elements):\n",
    "    \"\"\"\n",
    "    Filters out specific pairs in the correlation matrix based on a specified absolute value threshold,\n",
    "    removes pairs that share columns, removes symmetrical results, and returns the rows where the filter elements are found in either of the two variable columns.\n",
    "\n",
    "    Parameters:\n",
    "        - correlation_matrix (pandas.DataFrame): The correlation matrix.\n",
    "        - threshold (float): The absolute value threshold to filter the correlation coefficients.\n",
    "        - filter_elements (list): The elements to filter out from the correlation matrix.\n",
    "\n",
    "    Returns:\n",
    "        - filtered_pairs (pandas.DataFrame): The table showing the filtered pairs and their correlation coefficients,\n",
    "                                             where the filter elements are found in either of the two variable columns,\n",
    "                                             with symmetrical results removed.\n",
    "    \"\"\"\n",
    "    # Filter out pairs based on the threshold\n",
    "    filtered_pairs = correlation_matrix.unstack().reset_index()\n",
    "    filtered_pairs = filtered_pairs.rename(columns={'level_0': 'Variable A', 'level_1': 'Variable B', 0: 'Correlation'})\n",
    "    filtered_pairs = filtered_pairs[filtered_pairs['Correlation'].abs() >= threshold]\n",
    "\n",
    "    # Remove pairs that share columns\n",
    "    filtered_pairs = filtered_pairs[~(filtered_pairs['Variable A'] == filtered_pairs['Variable B'])]\n",
    "\n",
    "    # Remove symmetrical results\n",
    "    filtered_pairs = filtered_pairs[filtered_pairs['Variable A'] < filtered_pairs['Variable B']]\n",
    "\n",
    "    # Filter rows where the filter elements are found in either of the two variable columns\n",
    "    filtered_pairs = filtered_pairs[(filtered_pairs['Variable A'].isin(filter_elements)) | (filtered_pairs['Variable B'].isin(filter_elements))]\n",
    "    \n",
    "    # Sort the filtered pairs by correlation coefficient in descending order\n",
    "    filtered_pairs = filtered_pairs.sort_values('Correlation', ascending=False)\n",
    "    \n",
    "    filtered_pairs = filtered_pairs.reset_index()\n",
    "    \n",
    "\n",
    "    return filtered_pairs\n",
    "\n",
    "def check_column_exists(dataframes, column_name):\n",
    "    for df in dataframes:\n",
    "        if column_name in df.columns:\n",
    "            return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "505994c4-1e2d-442d-b90e-e269414aecd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10010, 751)\n",
      "(10010, 815)\n",
      "(10010, 1058)\n",
      "(10010, 737)\n",
      "(10010, 792)\n",
      "(10010, 1180)\n"
     ]
    }
   ],
   "source": [
    "#Test the script on one CSV\n",
    "#Load only reactions that have a flux higher than 0.0001 umol/m^2s?\n",
    "\n",
    "wt_250 = load_csv_and_cleanup('./flux_results/flux_sampling/flux_sample_WT_250_Relaxed_loopless_FVA_100kT.csv', tol = 1e-3)\n",
    "wt_750 = load_csv_and_cleanup('./flux_results/flux_sampling/flux_sample_WT_750_Relaxed_loopless_FVA_100kT_reran.csv', tol = 1e-3)\n",
    "wt_1500 = load_csv_and_cleanup('./flux_results/flux_sampling/flux_sample_WT_1500_Relaxed_loopless_FVA_100kT_reran.csv', tol = 1e-3)\n",
    "tr_250 = load_csv_and_cleanup('./flux_results/flux_sampling/flux_sample_TR_250_Relaxed_loopless_FVA_100kT.csv', tol = 1e-3)\n",
    "tr_750 = load_csv_and_cleanup('./flux_results/flux_sampling/flux_sample_TR_750_Relaxed_loopless_FVA_100kT.csv', tol = 1e-3)\n",
    "tr_1500 = load_csv_and_cleanup('./flux_results/flux_sampling/flux_sample_TR_1500_Relaxed_loopless_FVA_100kT.csv', tol = 1e-3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "df_list = [wt_250, wt_750, wt_1500, tr_250, tr_750, tr_1500]\n",
    "names = ['wt_250', 'wt_750', 'wt_1500', 'tr_250', 'tr_750', 'tr_1500']\n",
    "wt_list = [wt_250, wt_750, wt_1500]\n",
    "tr_list = [tr_250, tr_750, tr_1500]\n",
    "_250_list = [wt_250,tr_250]\n",
    "_750_list = [wt_750,tr_750]\n",
    "_1500_list=[wt_1500,tr_1500]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27838239-b1b8-4d2b-ac2e-af9e79d6f641",
   "metadata": {},
   "source": [
    "STAT NO. 2: Pairwise comparison of flux distributions\n",
    "This offers a more comprehensive view on which reactions have a different flux distribution in between samples. The test used for this stat is the Kruskal-Wallis ranked test, which is a non-parametric test typically used for flux sampling to compare fluxes between groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efd8f3df-2f8d-4d57-8ad4-d92c63892bb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wt_250_750_sig, wt_250_750_non_sig =compare_flux_distributions(wt_250, wt_750, significance_threshold=0.001)\n",
    "wt_250_1500_sig, wt_250_1500_non_sig =compare_flux_distributions(wt_250, wt_1500, significance_threshold=0.001)\n",
    "wt_750_1500_sig, wt_750_1500_non_sig =compare_flux_distributions(wt_750, wt_1500, significance_threshold=0.001)\n",
    "\n",
    "\n",
    "tr_250_750_sig, tr_250_750_non_sig = compare_flux_distributions(tr_250, tr_750, significance_threshold=0.001)\n",
    "tr_750_1500_sig, tr_750_1500_non_sig = compare_flux_distributions(tr_750, tr_1500, significance_threshold=0.001)\n",
    "tr_250_1500_sig, tr_250_1500_non_sig = compare_flux_distributions(tr_250, tr_1500, significance_threshold=0.001)\n",
    "\n",
    "wt_tr_250_sig, wt_tr_250_non_sig = compare_flux_distributions(wt_250, tr_250, significance_threshold=0.001)\n",
    "wt_tr_750_sig, wt_tr_750_non_sig = compare_flux_distributions(wt_750, tr_750, significance_threshold=0.001)\n",
    "wt_tr_1500_sig, wt_tr_1500_non_sig = compare_flux_distributions(wt_1500, tr_1500, significance_threshold=0.001)\n",
    "\n",
    "wt_invariant_fluxes = set(wt_250_750_non_sig) & set(wt_750_1500_non_sig)\n",
    "#no invariant fluxes between all samples WT\n",
    "\n",
    "tr_invariant_fluxes = set(tr_250_750_non_sig) & set(tr_750_1500_non_sig)\n",
    "#0 invariant fluxes\n",
    "\n",
    "wt_tr_250_750_invariant_fluxes = set(wt_tr_250_non_sig) & set(wt_tr_750_non_sig)\n",
    "#Only ATGS and CBMKs (Carbamate Kinase) as invariant fluxes detected\n",
    "#Includes reactions directed towards Star\n",
    "\n",
    "wt_tr_750_1500_invariant_fluxes = set(wt_tr_750_non_sig) & set(wt_tr_1500_non_sig)\n",
    "#Only proline PD trannsport detected as invariant @ high conditions\n",
    "\n",
    "\n",
    "wt_tr_250_1500_invariant_fluxes = set(wt_tr_250_non_sig) & set(wt_tr_1500_non_sig)\n",
    "#Invariant fluxes are related to photorespiratory export, interestingly enough. CO2 efflux from the Mitochondria, serine import and export from Mit. and Stroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b71bf-9f5f-446b-9cdd-33d094854e9d",
   "metadata": {},
   "source": [
    "#To Do for z-score plot\n",
    "- Get a list of all significant reactions found in each treatment comparison\n",
    "- Generate a dataframe containing z-scores for each reaction per treatment\n",
    "- Cluster dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e015b13-d3ac-4498-ac45-619a83f987c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersection(lists):\n",
    "    # Convert the first list to a set\n",
    "    intersection_set = set(lists[0])\n",
    "\n",
    "    # Iterate through the remaining lists and take the intersection with the set\n",
    "    for lst in lists[1:]:\n",
    "        intersection_set = intersection_set.intersection(lst)\n",
    "\n",
    "    # Convert the set back to a list and return it\n",
    "    intersection_list = list(intersection_set)\n",
    "    return intersection_list\n",
    "\n",
    "\n",
    "def get_all_elements(lists):\n",
    "    common_elements = set(lists[0])  # Initialize with the first list\n",
    "\n",
    "    # Iterate through the remaining lists and take the intersection with the common elements set\n",
    "    for lst in lists[1:]:\n",
    "        common_elements = common_elements.union(lst)\n",
    "\n",
    "    return list(common_elements)\n",
    "\n",
    "\n",
    "\n",
    "def fold_change_df(wt_list, tr_list, list_sigs):\n",
    "\n",
    "'''This function generates a list of ratios of the means of each dataframe for every parametrization. \n",
    "THe ratio is a value less than 0indicates a higher flux in the denominator  while a value higher than 0 indicates a higher relative flux in the numerator.\n",
    "I can either use log2FC or just the normal FC but I think it's better to use log2 to easily visualize the differences\n",
    " '''   \n",
    "    \n",
    "    def get_log2_fold_change(reaction, df1, df2):\n",
    "        sample1 = np.mean(df1[reaction]) if reaction in df1.columns else 1e-7\n",
    "        sample2 = np.mean(df2[reaction]) if reaction in df2.columns else 1e-7\n",
    "        \n",
    "        # print(reaction, sample1, sample2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        fold_change = np.log2(sample1/sample2)\n",
    "        \n",
    "        if np.isnan(fold_change)== True:\n",
    "            # print(f'{reaction} fold change is NaN') #This happens if one of the values is 0, then instead do the log subtraction form\n",
    "\n",
    "            \n",
    "            sample1 = np.log2(df1[reaction]+1e-7) if reaction in df1.columns else 1e-7\n",
    "            sample2 = np.log2(df2[reaction]+1e-7) if reaction in df2.columns else 1e-7\n",
    "            \n",
    "            sample1 = np.mean(sample1)\n",
    "            sample2 = np.mean(sample2)\n",
    "            \n",
    "            # print(sample1, sample2)\n",
    "            fold_change = sample1-sample2\n",
    "            # print(sample1, sample2, fold_change)\n",
    "            # print(f'new fold change: {fold_change}')\n",
    "        #Apparently using the log(sample1)-log(sample2) results with catastrophic subtraction so I think it's better to compute the log2 fold change directly from the means\n",
    "        #What I can do is instead\n",
    "        \n",
    "        return fold_change\n",
    "    \n",
    "    def get_fold_change(reaction, df1, df2): #This instead computes the ratio between samples A and B vs getting the log transformed form\n",
    "        sample1 = np.mean(df1[reaction] + 1e-7) if reaction in df1.columns else 1e-7\n",
    "        sample2 = np.mean(df2[reaction] + 1e-7) if reaction in df2.columns else 1e-7\n",
    "        \n",
    "        if sample1 == 1e-7 and sample2 == 1e-7:\n",
    "            fold_change =0\n",
    "        else:\n",
    "            fold_change = sample1/sample2\n",
    "        \n",
    "        return fold_change\n",
    "    #Returns fold-change if values exist, otherwise return None\n",
    "\n",
    "    #I think I should just compute the log-fold changes per reaction?\n",
    "\n",
    "    # Create an empty DataFrame with desired column names\n",
    "    \n",
    "    results_dict = dict()\n",
    "    #Generate values for result dictionary\n",
    "    \n",
    "    \n",
    "    for rxns in list_sigs:\n",
    "        _250_kw = get_log2_fold_change(rxns, wt_list[0], tr_list[0])\n",
    "        _750_kw = get_log2_fold_change(rxns, wt_list[1], tr_list[1])\n",
    "        _1500_kw = get_log2_fold_change(rxns, wt_list[2], tr_list[2])\n",
    "    \n",
    "        results_dict[rxns] = [_250_kw, _750_kw, _1500_kw]\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "    df.rename(columns={0: '250', 1: '750', 2: '1500'}, inplace=True)\n",
    "  \n",
    "    \n",
    "    return(df)\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def get_log2_fold_change(reaction, df1, df2):\n",
    "        sample1 = np.mean(df1[reaction]) if reaction in df1.columns else 1e-7\n",
    "        sample2 = np.mean(df2[reaction]) if reaction in df2.columns else 1e-7\n",
    "        \n",
    "        # print(reaction, sample1, sample2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        fold_change = np.log2(sample1/sample2)\n",
    "        \n",
    "        if np.isnan(fold_change)== True:\n",
    "            # print(f'{reaction} fold change is NaN') #This happens if one of the values is 0, then instead do the log subtraction form\n",
    "\n",
    "            \n",
    "            sample1 = np.log2(df1[reaction]+1e-7) if reaction in df1.columns else 1e-7\n",
    "            sample2 = np.log2(df2[reaction]+1e-7) if reaction in df2.columns else 1e-7\n",
    "            \n",
    "            sample1 = np.mean(sample1)\n",
    "            sample2 = np.mean(sample2)\n",
    "            \n",
    "            # print(sample1, sample2)\n",
    "            fold_change = sample1-sample2\n",
    "            # print(sample1, sample2, fold_change)\n",
    "            # print(f'new fold change: {fold_change}')\n",
    "        #Apparently using the log(sample1)-log(sample2) results with catastrophic subtraction so I think it's better to compute the log2 fold change directly from the means\n",
    "        #What I can do is instead\n",
    "        \n",
    "        return fold_change\n",
    "    \n",
    "    def get_fold_change(reaction, df1, df2): #This instead computes the ratio between samples A and B vs getting the log transformed form\n",
    "        sample1 = np.mean(df1[reaction] + 1e-7) if reaction in df1.columns else 1e-7\n",
    "        sample2 = np.mean(df2[reaction] + 1e-7) if reaction in df2.columns else 1e-7\n",
    "        \n",
    "        if sample1 == 1e-7 and sample2 == 1e-7:\n",
    "            fold_change =0\n",
    "        else:\n",
    "            fold_change = sample1/sample2\n",
    "        \n",
    "        return fold_change\n",
    "    #Returns fold-change if values exist, otherwise return None\n",
    "\n",
    "    #I think I should just compute the log-fold changes per reaction?\n",
    "\n",
    "    # Create an empty DataFrame with desired column names\n",
    "    \n",
    "    results_dict = dict()\n",
    "    #Generate values for result dictionary\n",
    "    \n",
    "    \n",
    "    for rxns in list_sigs:\n",
    "        _250_kw = get_log2_fold_change(rxns, wt_list[0], tr_list[0])\n",
    "        _750_kw = get_log2_fold_change(rxns, wt_list[1], tr_list[1])\n",
    "        _1500_kw = get_log2_fold_change(rxns, wt_list[2], tr_list[2])\n",
    "    \n",
    "        results_dict[rxns] = [_250_kw, _750_kw, _1500_kw]\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "    df.rename(columns={0: '250', 1: '750', 2: '1500'}, inplace=True)\n",
    "  \n",
    "    \n",
    "    return(df)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def get_stops(df, category_column):\n",
    "    '''#This function gets the stops and the groups for each dataframe passed. \n",
    "        It takes two arguments:\n",
    "        df: the dataframe being passed\n",
    "        category_columns: name of the column that corresponds to the subsystems of the dataframe\n",
    "    '''\n",
    "    \n",
    "    category_indices = []\n",
    "    \n",
    "    groups_list = df[category_column].unique()\n",
    "    \n",
    "    #reset the indices of each dataframe to get the numerical value\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    \n",
    "    #Iterate over the categories then retrieve the index of the \n",
    "    for category in groups_list:\n",
    "        \n",
    "        category_values = df[category_column].values\n",
    "        if category in category_values:\n",
    "            \n",
    "            start_index = df[df[category_column] == category].index.min()          \n",
    "            category_indices.append(start_index)\n",
    "            \n",
    "    #Append the last value of the index\n",
    "    category_indices.append(len(df))\n",
    "    \n",
    "    \n",
    "    return category_indices, groups_list\n",
    "            \n",
    "#This works now. This gets all the values of each\n",
    "def get_heatmap_range(df_list):\n",
    "    combined_data = pd.concat(df_list)\n",
    "    data_min = combined_data.min().min()\n",
    "    data_max = combined_data.max().max()\n",
    "\n",
    "    range = (max(abs(data_min), abs(data_max))) #Gets the nearest integer so it falls on a flat number\n",
    "    return(-range, range)\n",
    "    # return(data_min, data_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "681e2432-be90-465b-a46e-edfad94a929e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11011/2027308516.py:39: RuntimeWarning: invalid value encountered in log2\n",
      "  fold_change = np.log2(sample1/sample2)\n",
      "/home/articulatus/anaconda3/envs/RiceMM/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: invalid value encountered in log2\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'SUBSYSTEM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m reaction \u001b[38;5;241m=\u001b[39m trans_model\u001b[38;5;241m.\u001b[39mreactions\u001b[38;5;241m.\u001b[39mget_by_id(key)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Check if subsystem contains ';', in case it does split it and take only the first one\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mreaction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSUBSYSTEM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     21\u001b[0m     groups_dict[key] \u001b[38;5;241m=\u001b[39m reaction\u001b[38;5;241m.\u001b[39mnotes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSUBSYSTEM\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SUBSYSTEM'"
     ]
    }
   ],
   "source": [
    "sig_set = get_all_elements([wt_tr_250_sig, wt_tr_750_sig, wt_tr_1500_sig])\n",
    "sig_set_intersect = get_intersection([wt_tr_250_sig, wt_tr_750_sig, wt_tr_1500_sig])\n",
    "all_rxns_in_df = get_intersection([wt_250.columns, wt_750.columns, wt_1500.columns, tr_250.columns, tr_750.columns, tr_1500.columns])\n",
    "\n",
    "#This means that we are taking the log fold change between TR (Trans/Treated) vs WT(Control) so the log fold ratio is (Tr/WT)\n",
    "results_df = fold_change_df(tr_list, wt_list, sig_set)\n",
    "\n",
    "#Generate heat plot via each group in the model reaction group\n",
    "\n",
    "#First generate a dictionary for the grouping variable\n",
    "\n",
    "groups = [i for i in wt_model.groups]\n",
    "groups_dict = dict()\n",
    "\n",
    "\n",
    "for key in sig_set:\n",
    "    reaction = trans_model.reactions.get_by_id(key)\n",
    "    \n",
    "    # Check if subsystem contains ';', in case it does split it and take only the first one\n",
    "    if ';' in reaction.notes['SUBSYSTEM']:\n",
    "        groups_dict[key] = reaction.notes['SUBSYSTEM'].split(';')[0]\n",
    "    else:\n",
    "        groups_dict[key] = reaction.notes['SUBSYSTEM']\n",
    "\n",
    "    \n",
    "#Add the groups to the original dataframe\n",
    "results_df['group'] = results_df.index.map(groups_dict)\n",
    "    \n",
    "#Generate a dictionary corresponding to the order how to organize the reactions\n",
    "group_order = {}\n",
    "counter = 0\n",
    "\n",
    "for group in groups:\n",
    "    if group.id in groups_dict.values():\n",
    "        counter += 1\n",
    "        group_order[group.id] = counter\n",
    "        \n",
    "# #Afterwards generate a sorted dataframe using the group order shown above\n",
    "\n",
    "results_df['group_order'] = results_df['group'].map(group_order)\n",
    "results_df = results_df.sort_values(by='group_order').drop(columns='group_order')\n",
    "\n",
    "results_df_m =  results_df[results_df.index.str.contains('_M')]\n",
    "\n",
    "results_df_bs =  results_df[results_df.index.str.contains('_BS')]\n",
    "        \n",
    "\n",
    "#results_df now sorted. Generate a heatmap corresponding to the reactions now\n",
    "#Generate subset corresponding to heatmap data\n",
    "\n",
    "heatmap_data_m = results_df_m.iloc[:, :3]\n",
    "heatmap_data_bs = results_df_bs.iloc[:, :3]\n",
    "\n",
    "# list of places where the bands start and stop\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dca2607-76b9-4eb5-9a11-a5609c3ec305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try making a side-by-side heatmap showing both BS and M cells\n",
    "\n",
    "# Generate the heatmap\n",
    "\n",
    "\n",
    "#Generate color range from the original dataset \n",
    "vmin, vmax = get_heatmap_range([heatmap_data_m, heatmap_data_bs])\n",
    "\n",
    "#This is for the legends\n",
    "stops_m, group_list_m = get_stops(results_df_m, 'group')\n",
    "\n",
    "figsize= (5,100)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=figsize)\n",
    "\n",
    "sns.heatmap(heatmap_data_m, cmap='RdBu_r', annot=False,cbar=False, ax=ax[0], vmin=vmin, vmax=vmax)\n",
    "sns. set(font_scale=0.3) \n",
    "for beg, end, label in zip(stops_m[:-1], stops_m[1:], group_list_m):\n",
    "    ax[0].hlines(beg, 0, beg, color='black', lw=1)\n",
    "    ax[0].hlines(end, 0, -0.02, color='black', lw=1,\n",
    "              transform=ax[0].get_yaxis_transform(), clip_on=False)\n",
    "\n",
    "    # add some text to the center right of this band\n",
    "    ax[0].text(1.05, (beg + end) / 2, '\\n'.join(label.split()), ha='left', va='center', transform=ax[0].get_yaxis_transform())\n",
    "    \n",
    "    \n",
    "   #This is for the legends\n",
    "stops_bs, group_list_bs = get_stops(results_df_bs, 'group')\n",
    " \n",
    "\n",
    "    \n",
    "    \n",
    "sns.heatmap(heatmap_data_bs, cmap='RdBu_r', annot=False, cbar=False, ax=ax[1], vmin=vmin, vmax=vmax)\n",
    "\n",
    "for beg, end, label in zip(stops_bs[:-1], stops_bs[1:], group_list_bs):\n",
    "    ax[1].hlines(beg, 0, beg, color='black', lw=1)\n",
    "    ax[1].hlines(end, 0, -0.02, color='black', lw=1,\n",
    "              transform=ax[0].get_yaxis_transform(), clip_on=False)\n",
    "\n",
    "    # add some text to the center right of this band\n",
    "    ax[1].text(1.05, (beg + end) / 2, '\\n'.join(label.split()), ha='left', va='center', transform=ax[1].get_yaxis_transform())\n",
    "    \n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=1, hspace=3)\n",
    "\n",
    "    \n",
    "# plt.tight_layout()  # fit all text nicely into the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb2159c-b42d-4b5f-b655-cfa45ac9f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate the heatmap\n",
    "idx = heatmap_data_bs.columns\n",
    "\n",
    "\n",
    "#This is for the legends\n",
    "stops, group_list = get_stops(results_df_bs, 'group')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 100))\n",
    "\n",
    "sns.heatmap(heatmap_data_bs, cmap='RdBu_r', annot=False, cbar=True, ax=ax)\n",
    "\n",
    "for beg, end, label in zip(stops[:-1], stops[1:], group_list):\n",
    "    ax.hlines(beg, 0, beg, color='black', lw=1)\n",
    "    ax.hlines(end, 0, -0.02, color='black', lw=1,\n",
    "              transform=ax.get_yaxis_transform(), clip_on=False)\n",
    "\n",
    "    # add some text to the center right of this band\n",
    "    ax.text(1.05, (beg + end) / 2, '\\n'.join(label.split()), ha='left', va='center', transform=ax.get_yaxis_transform())\n",
    "plt.tight_layout()  # fit all text nicely into the plot\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592914ef-5101-42da-953b-211da4cc1728",
   "metadata": {},
   "source": [
    "Findings for the heat plot\n",
    "\n",
    "I don't think the heatplot of the log2 fold changes is too informative considering that the flux ranges are the actual meat of the analysis.\n",
    "\n",
    "What I should do is to prepare plots for all groups instead?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f21539-6766-4b9e-a786-50db796a6bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flux sampling at low, medium and high light regimens\n",
    "\n",
    "#Light treatments at 250 PPFD, 750 PPFD and 1500 PPFD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c25140-27e5-43eb-92f5-5af51ad9e06b",
   "metadata": {},
   "source": [
    "Correlation of flux measurements: use in designing experiments\n",
    "Uniform random sampling of the steady-state flux space allows for the calculation of the correlation coefficient (rij) between any two fluxes (vi and vj) in the network. Thus, sampling provides a straightforward means of not only calculating perfectly correlated subsets (\n",
    "), but also of identifying well-correlated, but not perfectly correlated reaction sets. The matrix of squared pairwise correlation coefficients for all the RBC metabolic fluxes was computed. The fluxes can be ordered such that the “correlated reaction sets” (defined here as \n",
    ") are listed in order of decreasing number of fluxes in each set (Table 1).\n",
    "\n",
    "\n",
    "From Price, Schellenberger and Palsson, (2004) -- Uniform Sammpling of flux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c306e-5065-4d39-96cd-938a4f6ec639",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Hmm. Interesting results. It shows that fluxes have become more decoupled in the low light regimens compared with medium and high light regimens when comparing results between parametrizations (WT and TR), at a threshold value of 0.001.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
