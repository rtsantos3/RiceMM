{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a656bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T11:08:41.205406Z",
     "start_time": "2023-03-23T11:08:39.074105Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-05-09\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os\n",
    "import cobra\n",
    "import cplex \n",
    "import libsbml\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import memote\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xlsxwriter\n",
    "import path \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from itertools import combinations\n",
    "from scipy.stats import kruskal, mannwhitneyu\n",
    "import multiprocessing\n",
    "from itertools import combinations, product\n",
    "from multiprocessing import Pool\n",
    "from sklearn.decomposition import PCA\n",
    "import mplcursors\n",
    "\n",
    "\n",
    "from sklearn.linear_model  import LinearRegression\n",
    "\n",
    "#Change working dir first, ty ChatGPT, much loves\n",
    "cwd = os.getcwd()\n",
    "# Split the path into a list of directories\n",
    "directories = cwd.split(os.sep)\n",
    "# Remove the last two directories from the list\n",
    "directories = directories[:-2]\n",
    "# Join the directories back into a path\n",
    "new_cwd = os.sep.join(directories)\n",
    "# Change the current working directory to the new path\n",
    "os.chdir(new_cwd)\n",
    "\n",
    "sys.path.append(\"./src/\")\n",
    "\n",
    "import model_initialize as mi\n",
    "import model_manipulation as mm\n",
    "\n",
    "\n",
    "#Set solver to gurobi\n",
    "config = cobra.Configuration()\n",
    "config.solver = 'gurobi'\n",
    "\n",
    "#Read 2-cell model\n",
    "wt_model = cobra.io.read_sbml_model(\"./model/ios2164_2cell.xml\")\n",
    "trans_model = cobra.io.read_sbml_model(\"./model/ios2164_2cell.xml\")\n",
    "#Estimate inf\n",
    "inf = 1e6\n",
    "\n",
    "\n",
    "#Add trans reactions to trans_model\n",
    "mi.add_trans_reactions(trans_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b7ca3-4c3d-4250-98f8-d84c422a5a39",
   "metadata": {},
   "source": [
    "Pipeline breakdown:\n",
    "\n",
    "Load CSVs to a memory saving format first\n",
    "\n",
    "1.\n",
    "Run convergence statistics on each and generate plots to assess total convergence stats for each CSV. These will include tests such as the Geweke statistic. \n",
    "Afterwards get only the flux names of those reactions that have converged\n",
    "Run pairwise Kruskal-wallis tests per CSV using the above list of converged reactions\n",
    "Identify each reaction with significant and non-significant distributions each\n",
    "\n",
    "Generate histograms/probability densities for relevant reactions with significantly different distributions with WT and Trans models\n",
    "\n",
    "2. Flux coupling analysis\n",
    "Check which fluxes are coupled with each otehr and identify which fluxes are then related to each other, particularly Carbon Fixation reactions in the BS cell such as Rubisco and the DM_Phloem reactions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "405afc4c-5980-4e21-b14f-e3045ac097e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This code block contains functions that we need to use to analyze our flux sampling experiment. \n",
    "These include tests to check for convergence, for autocorrelation, as well as for pairwise comparisons of fluxes between\n",
    "parametization regimens (WT, TR, as well as 3 light conditions) and a script to determine whether fluxes are\n",
    "coupled with each other. Lastly, I also tried PCA to determine which reactions contribute most to the variance of each null space distribution per\n",
    "sampling run.\n",
    "'''\n",
    "\n",
    "def get_df_name(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    return name\n",
    "\n",
    "\n",
    "def load_csv_and_cleanup(filename, tol=1e-7):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Remove columns with a mean below tolerance\n",
    "    mean_values = df.mean()\n",
    "    columns_to_remove = mean_values[abs(mean_values) < tol].index\n",
    "    df = df.drop(columns=columns_to_remove)\n",
    "\n",
    "    # Fix the indices\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(df.shape)\n",
    "\n",
    "    return df    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def compare_pair(reaction, df1, df2, significance_threshold):\n",
    "    \"\"\"\n",
    "    Compare the flux distributions of a pair of columns from two dataframes using the Kruskal-Wallis test.\n",
    "\n",
    "    :param pair: Tuple containing the pair of column names.\n",
    "    :param df1: The first dataframe.\n",
    "    :param df2: The second dataframe.\n",
    "    :param significance_threshold: The significance threshold to use for the Kruskal-Wallis test.\n",
    "    :return: The pair and the result of the Kruskal-Wallis test.\n",
    "    \"\"\"\n",
    "    sample1 = df1[reaction]\n",
    "    sample2 = df2[reaction]\n",
    "    H, pval = kruskal(sample1, sample2)\n",
    "    return pval \n",
    "\n",
    "def compare_flux_distributions(df1, df2, significance_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Compare the flux distributions of each pair of columns from two dataframes using the Kruskal-Wallis test.\n",
    "\n",
    "    :param df1: The first dataframe.\n",
    "    :param df2: The second dataframe.\n",
    "    :param significance_threshold: The significance threshold to use for the Kruskal-Wallis test (default: 0.05).\n",
    "    :return: A list of pairs with significantly different distributions.\n",
    "    \"\"\"\n",
    "    common_columns = set(df1.columns) & set(df2.columns)\n",
    "    \n",
    "    pool = multiprocessing.Pool()\n",
    "    sig_results = []\n",
    "    non_sig_results = []\n",
    "    \n",
    "    for rxn in common_columns:\n",
    "        kw_pval = compare_pair(rxn, df1, df2, significance_threshold) #This does the Kruskal-Wallis part\n",
    "        if kw_pval < significance_threshold:\n",
    "            sig_results.append(rxn)\n",
    "        else:\n",
    "            non_sig_results.append(rxn)\n",
    "            \n",
    "    #Sort the outputs before returning\n",
    "    sig_results.sort()\n",
    "    non_sig_results.sort()\n",
    "            \n",
    "    \n",
    "    return sig_results, non_sig_results\n",
    "\n",
    "\n",
    "\n",
    "#The script above is used for flux coupling using the opened CSV file as input and outputs 3 lists:\n",
    "#positively correlated, neg. correlated and uncoupled reactions\n",
    "\n",
    "\n",
    "#This is for visualization\n",
    "def generate_histograms(dataframe1, dataframe2, column_name):\n",
    "    # Create subplots for two histograms\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "    # Histogram for dataframe1\n",
    "    axes[0].hist(dataframe1[column_name], bins=10, color='skyblue')\n",
    "    axes[0].set_title(f'Histogram of {column_name}')\n",
    "    axes[0].set_xlabel(column_name)\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    # Histogram for dataframe2\n",
    "    axes[1].hist(dataframe2[column_name], bins=10, color='lightgreen')\n",
    "    axes[1].set_title(f'Histogram of {column_name}')\n",
    "    axes[1].set_xlabel(column_name)\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the histograms\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def generate_stacked_histogram(dataframes, column_name,histtype='bar'):\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    # Concatenate the column data from all dataframes into a single series\n",
    "    \n",
    "    df_name = list()\n",
    "    \n",
    "    \n",
    "    for dfs in dataframes:\n",
    "    # Create the stacked histogram\n",
    "        if column_name in dfs.columns:\n",
    "\n",
    "            plt.hist(dfs[column_name], bins=33, alpha=0.45, stacked=True, density=False,histtype=histtype)\n",
    "            df_name.append(get_df_name(dfs))\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Set plot title and labels\n",
    "    plt.title(f\"Stacked Histogram for Column: {column_name}\")\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend(df_name)\n",
    " \n",
    "    \n",
    "    plt.figure().set_figheight(1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_scatter_regression(df, column_x, column_y):\n",
    "    \"\"\"\n",
    "    Generates a scatterplot and a linear regression line between two columns in a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        - df (pandas.DataFrame): The input dataframe.\n",
    "        - column_x (str): The column name for the X-axis.\n",
    "        - column_y (str): The column name for the Y-axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract X and Y values from the dataframe\n",
    "    X = df[column_x].values.reshape(-1, 1)\n",
    "    Y = df[column_y].values\n",
    "\n",
    "    # Fit linear regression model\n",
    "    regression = LinearRegression()\n",
    "    regression.fit(X, Y)\n",
    "\n",
    "    # Predict Y values based on the regression line\n",
    "    Y_pred = regression.predict(X)\n",
    "\n",
    "    # Plot the scatterplot and regression line\n",
    "    plt.scatter(X, Y, color='blue', label='Actual')\n",
    "    plt.plot(X, Y_pred, color='red', label='Regression Line')\n",
    "    plt.xlabel(column_x)\n",
    "    plt.ylabel(column_y)\n",
    "    plt.title('Scatterplot with Linear Regression Line')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#These functions are for generating PCA plots and other corollary analyses\n",
    "\n",
    "def merge_dataframes(dataframes, names):\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    for i, df in enumerate(dataframes):\n",
    "        # Get the corresponding name from the names list\n",
    "        df_name = names[i]\n",
    "        \n",
    "        # Add a new column with the dataframe name\n",
    "        df['sample'] = df_name\n",
    "        \n",
    "        # Merge dataframes\n",
    "        if merged_df.empty:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "    \n",
    "    # Remove NAs\n",
    "    merged_df = merged_df.fillna(1e-7)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def generate_pca_plot(dataframe):\n",
    "    numeric_columns = dataframe.select_dtypes(include=[np.number]).columns\n",
    "    X = dataframe[numeric_columns].values\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "    components = pca.fit_transform(X)\n",
    "\n",
    "    # Extract dataframe names\n",
    "    dataframe_names = dataframe['sample'].unique()\n",
    "\n",
    "    # Generate plot with different colors for each dataframe\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'yellow']  # Add more colors if needed\n",
    "\n",
    "    #Generate figure and subplot\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    for i, name in enumerate(dataframe_names):\n",
    "        indices = dataframe['sample'] == name\n",
    "        ax.scatter(components[indices, 0], components[indices, 1], label=name, c=colors[i])\n",
    "\n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "    ax.legend()\n",
    "    \n",
    "    mplcursors.cursor(hover=True)\n",
    "    plt.show()\n",
    " \n",
    "    #export PCA to determine component variance\n",
    "    \n",
    "    pca_fit = pca.fit(X)\n",
    "    \n",
    "    return pca_fit\n",
    "\n",
    "def select_significant_features(pca, feature_names, top_n):\n",
    "    loadings = pca.components_\n",
    "    abs_loadings = np.abs(loadings)\n",
    "    feature_contribution = np.sum(abs_loadings, axis=0)\n",
    "    sorted_features = feature_names[np.argsort(feature_contribution)[::-1]]\n",
    "    selected_features = sorted_features[:top_n]\n",
    "    return selected_features\n",
    "\n",
    "def plot_selected_features_heatmap(pca, feature_names, selected_features):\n",
    "    loadings = pca.components_\n",
    "    selected_indices = [np.where(feature_names == feature)[0][0] for feature in selected_features]\n",
    "    selected_loadings = loadings[:, selected_indices]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(selected_loadings, cmap='coolwarm', annot=False, xticklabels=selected_features)\n",
    "    plt.xlabel('Selected Features')\n",
    "    plt.ylabel('Principal Components')\n",
    "    plt.title('Loadings Heatmap of Selected Features')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "def visualize_correlation_matrix(df, sort_by=None, ascending=False):\n",
    "    \"\"\"\n",
    "    Calculates and visualizes the correlation matrix based on flux measurements in a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        - df (pandas.DataFrame): The input dataframe containing flux measurements.\n",
    "        - sort_by (str or None): Column name to sort the correlation matrix by. Default is None.\n",
    "        - ascending (bool): Whether to sort in ascending order. Default is False.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = df.corr()\n",
    "\n",
    "    # Sort the correlation matrix if sort_by is specified\n",
    "    if sort_by is not None:\n",
    "        correlation_matrix = correlation_matrix.sort_values(by=sort_by, ascending=ascending)\n",
    "\n",
    "    # Create a heatmap of the correlation matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True)\n",
    "\n",
    "    # Set plot title\n",
    "    plt.title('Correlation Matrix')\n",
    "\n",
    "    # Display the heatmap\n",
    "    plt.show()\n",
    "\n",
    "def filter_correlation_pairs(correlation_matrix, threshold, filter_elements):\n",
    "    \"\"\"\n",
    "    Filters out specific pairs in the correlation matrix based on a specified absolute value threshold,\n",
    "    removes pairs that share columns, removes symmetrical results, and returns the rows where the filter elements are found in either of the two variable columns.\n",
    "\n",
    "    Parameters:\n",
    "        - correlation_matrix (pandas.DataFrame): The correlation matrix.\n",
    "        - threshold (float): The absolute value threshold to filter the correlation coefficients.\n",
    "        - filter_elements (list): The elements to filter out from the correlation matrix.\n",
    "\n",
    "    Returns:\n",
    "        - filtered_pairs (pandas.DataFrame): The table showing the filtered pairs and their correlation coefficients,\n",
    "                                             where the filter elements are found in either of the two variable columns,\n",
    "                                             with symmetrical results removed.\n",
    "    \"\"\"\n",
    "    # Filter out pairs based on the threshold\n",
    "    filtered_pairs = correlation_matrix.unstack().reset_index()\n",
    "    filtered_pairs = filtered_pairs.rename(columns={'level_0': 'Variable A', 'level_1': 'Variable B', 0: 'Correlation'})\n",
    "    filtered_pairs = filtered_pairs[filtered_pairs['Correlation'].abs() >= threshold]\n",
    "\n",
    "    # Remove pairs that share columns\n",
    "    filtered_pairs = filtered_pairs[~(filtered_pairs['Variable A'] == filtered_pairs['Variable B'])]\n",
    "\n",
    "    # Remove symmetrical results\n",
    "    filtered_pairs = filtered_pairs[filtered_pairs['Variable A'] < filtered_pairs['Variable B']]\n",
    "\n",
    "    # Filter rows where the filter elements are found in either of the two variable columns\n",
    "    filtered_pairs = filtered_pairs[(filtered_pairs['Variable A'].isin(filter_elements)) | (filtered_pairs['Variable B'].isin(filter_elements))]\n",
    "    \n",
    "    # Sort the filtered pairs by correlation coefficient in descending order\n",
    "    filtered_pairs = filtered_pairs.sort_values('Correlation', ascending=False)\n",
    "    \n",
    "    filtered_pairs = filtered_pairs.reset_index()\n",
    "    \n",
    "\n",
    "    return filtered_pairs\n",
    "\n",
    "def check_column_exists(dataframes, column_name):\n",
    "    for df in dataframes:\n",
    "        if column_name in df.columns:\n",
    "            return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "505994c4-1e2d-442d-b90e-e269414aecd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10010, 1214)\n",
      "(10010, 1705)\n",
      "(10010, 1782)\n",
      "(10010, 1157)\n",
      "(10010, 1113)\n",
      "(10010, 1289)\n"
     ]
    }
   ],
   "source": [
    "#Test the script on one CSV\n",
    "\n",
    "wt_250 = load_csv_and_cleanup('./flux_results/flux_sampling/flux_sample_WT_250_Relaxed_loopless_FVA_100kT.csv', )\n",
    "wt_750 = load_csv_and_cleanup('./flux_results/flux_sampling/flux_sample_WT_750_Relaxed_loopless_FVA_100kT.csv')\n",
    "wt_1500 = load_csv_and_cleanup('./flux_results/flux_sampling/flux_sample_WT_1500_Relaxed_loopless_FVA_100kT.csv')\n",
    "tr_250 = load_csv_and_cleanup('./flux_results/flux_sampling/flux_sample_TR_250_Relaxed_loopless_FVA_100kT.csv')\n",
    "tr_750 = load_csv_and_cleanup('./flux_results/flux_sampling/flux_sample_TR_750_Relaxed_loopless_FVA_100kT.csv')\n",
    "tr_1500 = load_csv_and_cleanup('./flux_results/flux_sampling/flux_sample_TR_1500_Relaxed_loopless_FVA_100kT.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "df_list = [wt_250, wt_750, wt_1500, tr_250, tr_750, tr_1500]\n",
    "names = ['wt_250', 'wt_750', 'wt_1500', 'tr_250', 'tr_750', 'tr_1500']\n",
    "wt_list = [wt_250, wt_750, wt_1500]\n",
    "tr_list = [tr_250, tr_750, tr_1500]\n",
    "_250_list = [wt_250,tr_250]\n",
    "_750_list = [wt_750,tr_750]\n",
    "_1500_list=[wt_1500,tr_1500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36e463ab-4239-4f1d-b4e8-aeb7e2459793",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_means_summary(df_list, df_names, filename, path):\n",
    "    writer = pd.ExcelWriter(f'{path}/{filename}', engine='xlsxwriter')\n",
    "\n",
    "    for i in range(len(df_list)):\n",
    "        df = df_list[i]\n",
    "        name = df_names[i]\n",
    "\n",
    "        # Compute means per column\n",
    "        means = df.mean()\n",
    "\n",
    "        # Compute standard deviations per column\n",
    "        std_devs = df.std()\n",
    "\n",
    "        # Combine means and standard deviations into a single dataframe\n",
    "        combined_df = pd.concat([means, std_devs], axis=1)\n",
    "\n",
    "        # Write combined dataframe to separate sheet\n",
    "        combined_df.to_excel(writer, sheet_name=name)\n",
    "\n",
    "    # Save the workbook\n",
    "    writer.save()\n",
    "    print(f'Workbook saved to {path}/{filename}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d99b9b08-3432-4d0f-a446-fb18eddcac36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking  wt_250  for loops:\n",
      "Checking  wt_750  for loops:\n",
      "Index name: UDPG4Ec_M, Mean value: 7801.837144882489\n",
      "Index name: INSPc_M, Mean value: 18893.514449201597\n",
      "Index name: ADNDAc_M, Mean value: 18893.38801523989\n",
      "Index name: DAD2DAc_M, Mean value: 122320.49421899393\n",
      "Index name: DINSPc_M, Mean value: 122320.49421899393\n",
      "Index name: MTHFCs_M, Mean value: 199225.46973610905\n",
      "Index name: MTHFCm_M, Mean value: 480103.9100155868\n",
      "Index name: FPGFTs_M, Mean value: 199225.4452322195\n",
      "Index name: FGFTs_M, Mean value: 199225.4215350426\n",
      "Index name: FPGFTm_M, Mean value: 480103.9100155868\n",
      "Index name: FGFTm_M, Mean value: 480103.9100155868\n",
      "Index name: FA140ACPHs_M, Mean value: 188925.07588037377\n",
      "Index name: FA160ACPHs_M, Mean value: 486303.2223416938\n",
      "Index name: FA161ACPHs_M, Mean value: 513259.7858539757\n",
      "Index name: FA180ACPHs_M, Mean value: 493748.36554022186\n",
      "Index name: FA181ACPHs_M, Mean value: 173492.6040655187\n",
      "Index name: FA140ACOASc_M, Mean value: 188925.07588037377\n",
      "Index name: FA160ACOASc_M, Mean value: 486303.2223416938\n",
      "Index name: FA161ACOASc_M, Mean value: 513259.7858539757\n",
      "Index name: FA180ACOASc_M, Mean value: 493748.36083258037\n",
      "Index name: FA181ACOASc_M, Mean value: 173492.60529969496\n",
      "Index name: XYL2c_BS, Mean value: 23382.12793860784\n",
      "Index name: INSPc_BS, Mean value: 24033.621752613173\n",
      "Index name: ADNDAc_BS, Mean value: 24033.48163915023\n",
      "Index name: DAD2DAc_BS, Mean value: 159514.70111557236\n",
      "Index name: DINSPc_BS, Mean value: 159514.70111557236\n",
      "Index name: MTHFCm_BS, Mean value: 93082.78392702424\n",
      "Index name: ADE1c_BS, Mean value: 5779.585812871415\n",
      "Index name: FPGFTm_BS, Mean value: 93082.78392702424\n",
      "Index name: FGFTm_BS, Mean value: 93082.78392702424\n",
      "Index name: FA140ACPHs_BS, Mean value: 60941.75557844324\n",
      "Index name: FA160ACPHs_BS, Mean value: 505082.98367929086\n",
      "Index name: FA161ACPHs_BS, Mean value: 476736.4448330505\n",
      "Index name: FA180ACPHs_BS, Mean value: 504347.07077850954\n",
      "Index name: FA181ACPHs_BS, Mean value: 49517.49609385111\n",
      "Index name: FA140ACOASc_BS, Mean value: 60941.75557844324\n",
      "Index name: FA160ACOASc_BS, Mean value: 505082.98367929086\n",
      "Index name: FA161ACOASc_BS, Mean value: 476736.4448330505\n",
      "Index name: FA180ACOASc_BS, Mean value: 504347.06923151977\n",
      "Index name: FA181ACOASc_BS, Mean value: 49517.49700199348\n",
      "Index name: glc-A_pd, Mean value: 29161.727474873976\n",
      "Index name: udpgal_pd, Mean value: 7798.278728579409\n",
      "Checking  wt_1500  for loops:\n",
      "Index name: XYL2c_M, Mean value: 4198.312015029678\n",
      "Index name: INSPc_M, Mean value: 18192.917263583506\n",
      "Index name: ADNDAc_M, Mean value: 18192.917263492935\n",
      "Index name: DAD2DAc_M, Mean value: 187352.0504426499\n",
      "Index name: DINSPc_M, Mean value: 187352.0504426499\n",
      "Index name: MTHFCs_M, Mean value: 233165.3080910975\n",
      "Index name: MTHFCm_M, Mean value: 492168.16038545425\n",
      "Index name: FPGFTs_M, Mean value: 233165.23877200967\n",
      "Index name: FGFTs_M, Mean value: 233165.1721308636\n",
      "Index name: FPGFTm_M, Mean value: 492168.16038545425\n",
      "Index name: FGFTm_M, Mean value: 492168.16038545425\n",
      "Index name: FA140ACPHs_M, Mean value: 196335.74582344852\n",
      "Index name: FA160ACPHs_M, Mean value: 497085.76468342904\n",
      "Index name: FA161ACPHs_M, Mean value: 506021.21610519255\n",
      "Index name: FA180ACPHs_M, Mean value: 502718.9087730561\n",
      "Index name: FA181ACPHs_M, Mean value: 494205.1052249076\n",
      "Index name: FA140ACOASc_M, Mean value: 196335.74582344852\n",
      "Index name: FA160ACOASc_M, Mean value: 497085.76468342904\n",
      "Index name: FA161ACOASc_M, Mean value: 506021.21610519255\n",
      "Index name: FA180ACOASc_M, Mean value: 502718.9087730561\n",
      "Index name: FA181ACOASc_M, Mean value: 494205.1052249076\n",
      "Index name: UDPG4Ec_BS, Mean value: 109560.5111234816\n",
      "Index name: INSPc_BS, Mean value: 22967.286789999158\n",
      "Index name: ADNDAc_BS, Mean value: 22967.289860576093\n",
      "Index name: DAD2DAc_BS, Mean value: 216398.15807407352\n",
      "Index name: DINSPc_BS, Mean value: 216398.15807407352\n",
      "Index name: MTHFCm_BS, Mean value: 98527.56525038043\n",
      "Index name: ADE1c_BS, Mean value: 69452.31847821888\n",
      "Index name: FPGFTm_BS, Mean value: 98527.56525038043\n",
      "Index name: FGFTm_BS, Mean value: 98527.56525038043\n",
      "Index name: FA140ACPHs_BS, Mean value: 71785.70024856865\n",
      "Index name: FA160ACPHs_BS, Mean value: 42155.65502008364\n",
      "Index name: FA161ACPHs_BS, Mean value: 497188.77452503855\n",
      "Index name: FA180ACPHs_BS, Mean value: 509771.6328973402\n",
      "Index name: FA181ACPHs_BS, Mean value: 500144.45798495243\n",
      "Index name: FA140ACOASc_BS, Mean value: 71785.6998818533\n",
      "Index name: FA160ACOASc_BS, Mean value: 42155.65238034959\n",
      "Index name: FA161ACOASc_BS, Mean value: 497188.77452503855\n",
      "Index name: FA180ACOASc_BS, Mean value: 509771.62235858984\n",
      "Index name: FA181ACOASc_BS, Mean value: 500144.45771171444\n",
      "Index name: glc-A_pd, Mean value: 65255.138901208986\n",
      "Index name: fru-B_pd, Mean value: 4197.490287458852\n",
      "Index name: udpg_pd, Mean value: 109557.98394792285\n",
      "Checking  tr_250  for loops:\n",
      "Checking  tr_750  for loops:\n",
      "Checking  tr_1500  for loops:\n"
     ]
    }
   ],
   "source": [
    "loops_list = list()\n",
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "    \n",
    "    means = df.mean()\n",
    "    print('Checking ', names[i], ' for loops:')\n",
    "    for index, m in means.items():\n",
    "        if m > 3000:\n",
    "            print(f\"Index name: {index}, Mean value: {m}\")\n",
    "            if index not in loops_list:\n",
    "                loops_list.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6fdb2532-5c00-432e-9d23-633f4f918913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UDPG4Ec_M | UDP-glucose 4-epimerase | Primary Cell Wall Metabolism (Galactose metabolism)\n",
      "INSPc_M | inosine phosphorylase | Purine metabolism\n",
      "ADNDAc_M | adenosine deaminase (polypeptide) | Purine metabolism\n",
      "DAD2DAc_M | Deoxyadenosine aminohydrolase | Purine metabolism\n",
      "DINSPc_M | Deoxyinosine:orthophosphate ribosyltransferase | Purine metabolism\n",
      "MTHFCs_M | Methenyltetrahydrofolate cyclohydrolase, plastidic | Folates metabolism\n",
      "MTHFCm_M | Methenyltetrahydrofolate cyclohydrolase, mitochondrial | Folates metabolism\n",
      "FPGFTs_M | Phosphoribosylglycinamide formyltransferase | Purine metabolism\n",
      "FGFTs_M | N2-Formyl-N1-(5-phospho-D-ribosyl)glycinamide formyltransferase, chloroplast | Purine metabolism\n",
      "FPGFTm_M | Phosphoribosylglycinamide formyltransferase | Purine metabolism\n",
      "FGFTm_M | N2-Formyl-N1-(5-phospho-D-ribosyl)glycinamide formyltransferase, mitochondria | Purine metabolism\n",
      "FA140ACPHs_M | fatty-acyl-ACP hydrolase (n-C14:0) | Fatty acid biosynthesis\n",
      "FA160ACPHs_M | fatty-acyl-ACP hydrolase (n-C16:0) | Fatty acid biosynthesis\n",
      "FA161ACPHs_M | fatty-acyl-ACP hydrolase (n-C16:1) | Fatty acid biosynthesis\n",
      "FA180ACPHs_M | fatty-acyl-ACP hydrolase (n-C18:0) | Fatty acid biosynthesis\n",
      "FA181ACPHs_M | fatty-acyl-ACP hydrolase (n-C18:1) | Fatty acid biosynthesis\n",
      "FA140ACOASc_M | long-chain-fatty-acid-CoA ligase (n-C14:0) | Fatty acid biosynthesis\n",
      "FA160ACOASc_M | long-chain-fatty-acid-CoA ligase (n-C16:0) | Fatty acid biosynthesis\n",
      "FA161ACOASc_M | long-chain-fatty-acid-CoA ligase (n-C16:1) | Fatty acid biosynthesis\n",
      "FA180ACOASc_M | long-chain-fatty-acid-CoA ligase (n-C18:0) | Fatty acid biosynthesis\n",
      "FA181ACOASc_M | long-chain-fatty-acid-CoA ligase (n-C18:1) | Fatty acid biosynthesis\n",
      "XYL2c_BS | Xylose isomerase, cytosolic | Primary Cell Wall Metabolism (Fructose)\n",
      "INSPc_BS | inosine phosphorylase | Purine metabolism\n",
      "ADNDAc_BS | adenosine deaminase (polypeptide) | Purine metabolism\n",
      "DAD2DAc_BS | Deoxyadenosine aminohydrolase | Purine metabolism\n",
      "DINSPc_BS | Deoxyinosine:orthophosphate ribosyltransferase | Purine metabolism\n",
      "MTHFCm_BS | Methenyltetrahydrofolate cyclohydrolase, mitochondrial | Folates metabolism\n",
      "ADE1c_BS | Aldose 1-epimerase, cytosolic | Glycolysis/Gluconeogensis\n",
      "FPGFTm_BS | Phosphoribosylglycinamide formyltransferase | Purine metabolism\n",
      "FGFTm_BS | N2-Formyl-N1-(5-phospho-D-ribosyl)glycinamide formyltransferase, mitochondria | Purine metabolism\n",
      "FA140ACPHs_BS | fatty-acyl-ACP hydrolase (n-C14:0) | Fatty acid biosynthesis\n",
      "FA160ACPHs_BS | fatty-acyl-ACP hydrolase (n-C16:0) | Fatty acid biosynthesis\n",
      "FA161ACPHs_BS | fatty-acyl-ACP hydrolase (n-C16:1) | Fatty acid biosynthesis\n",
      "FA180ACPHs_BS | fatty-acyl-ACP hydrolase (n-C18:0) | Fatty acid biosynthesis\n",
      "FA181ACPHs_BS | fatty-acyl-ACP hydrolase (n-C18:1) | Fatty acid biosynthesis\n",
      "FA140ACOASc_BS | long-chain-fatty-acid-CoA ligase (n-C14:0) | Fatty acid biosynthesis\n",
      "FA160ACOASc_BS | long-chain-fatty-acid-CoA ligase (n-C16:0) | Fatty acid biosynthesis\n",
      "FA161ACOASc_BS | long-chain-fatty-acid-CoA ligase (n-C16:1) | Fatty acid biosynthesis\n",
      "FA180ACOASc_BS | long-chain-fatty-acid-CoA ligase (n-C18:0) | Fatty acid biosynthesis\n",
      "FA181ACOASc_BS | long-chain-fatty-acid-CoA ligase (n-C18:1) | Fatty acid biosynthesis\n",
      "glc-A_pd | Plasmodesmatal transport of alpha-D-glucose | Plasmodesmatal Transport\n",
      "udpgal_pd | Plasmodesmatal transport of UDP-galactose | Plasmodesmatal Transport\n",
      "XYL2c_M | Xylose isomerase, cytosolic | Primary Cell Wall Metabolism (Fructose)\n",
      "UDPG4Ec_BS | UDP-glucose 4-epimerase | Primary Cell Wall Metabolism (Galactose metabolism)\n",
      "fru-B_pd | Plasmodesmatal transport of beta-D-fructose | Plasmodesmatal Transport\n",
      "udpg_pd | Plasmodesmatal transport of UDP-glucose | Plasmodesmatal Transport\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for loops in loops_list:\n",
    "    subsystem = wt_model.reactions.get_by_id(loops).notes['SUBSYSTEM']\n",
    "    reaction_name =wt_model.reactions.get_by_id(loops).name\n",
    "    print(f'{loops} | {reaction_name} | {subsystem}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f8a2e9-b7f8-4260-b482-3f4e93dda89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workbook saved to ./flux_results/flux_sampling/sampling_stats//sampling_means_summary.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16321/2924371692.py:21: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
      "  writer.save()\n"
     ]
    }
   ],
   "source": [
    "generate_means_summary(df_list, names, filename='sampling_means_summary.xlsx', path='./flux_results/flux_sampling/sampling_stats/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c4d9f1-a1aa-4782-b04c-59f6e3d5f5ef",
   "metadata": {},
   "source": [
    "June 28, 2023\n",
    "\n",
    "RESULTS:\n",
    "\n",
    "WT750 and WT1500 needs to be rerun due to numerical issues in the final sampling.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
