{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03a656bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T11:08:41.205406Z",
     "start_time": "2023-03-23T11:08:39.074105Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-05-09\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os\n",
    "import cobra\n",
    "import cplex \n",
    "import libsbml\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import memote\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xlsxwriter\n",
    "import path \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from itertools import combinations\n",
    "from scipy.stats import kruskal, mannwhitneyu\n",
    "import multiprocessing\n",
    "from itertools import combinations, product\n",
    "from multiprocessing import Pool\n",
    "from sklearn.decomposition import PCA\n",
    "import mplcursors\n",
    "import math\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "\n",
    "from sklearn.linear_model  import LinearRegression\n",
    "\n",
    "#Change working dir first, ty ChatGPT, much loves\n",
    "cwd = os.getcwd()\n",
    "# Split the path into a list of directories\n",
    "directories = cwd.split(os.sep)\n",
    "# Remove the last two directories from the list\n",
    "directories = directories[:-2]\n",
    "# Join the directories back into a path\n",
    "new_cwd = os.sep.join(directories)\n",
    "# Change the current working directory to the new path\n",
    "os.chdir(new_cwd)\n",
    "\n",
    "sys.path.append(\"./src/\")\n",
    "\n",
    "import model_initialize as mi\n",
    "import model_manipulation as mm\n",
    "\n",
    "\n",
    "#Set solver to gurobi\n",
    "config = cobra.Configuration()\n",
    "config.solver = 'gurobi'\n",
    "\n",
    "#Read 2-cell model\n",
    "wt_model = cobra.io.read_sbml_model(\"./model/ios2164_2cell.xml\")\n",
    "trans_model = cobra.io.read_sbml_model(\"./model/ios2164_2cell.xml\")\n",
    "#Estimate inf\n",
    "inf = 1e6\n",
    "\n",
    "\n",
    "#Add trans reactions to trans_model\n",
    "mi.add_trans_reactions(trans_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b7ca3-4c3d-4250-98f8-d84c422a5a39",
   "metadata": {
    "tags": []
   },
   "source": [
    "Pipeline breakdown:\n",
    "\n",
    "Load CSVs to a memory saving format first\n",
    "\n",
    "1.\n",
    "Run convergence statistics on each and generate plots to assess total convergence stats for each CSV. These will include tests such as the Geweke statistic. \n",
    "Afterwards get only the flux names of those reactions that have converged\n",
    "Run pairwise Kruskal-wallis tests per CSV using the above list of converged reactions\n",
    "Identify each reaction with significant and non-significant distributions each\n",
    "\n",
    "Generate histograms/probability densities for relevant reactions with significantly different distributions with WT and Trans models\n",
    "\n",
    "2. Flux coupling analysis\n",
    "Check which fluxes are coupled with each otehr and identify which fluxes are then related to each other, particularly Carbon Fixation reactions in the BS cell such as Rubisco and the DM_Phloem reactions. \n",
    "\n",
    "\n",
    "Visualization of fluxes\n",
    "1. Violin plots for each constituent reaction group showing WT and TR per light parameter\n",
    "2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "405afc4c-5980-4e21-b14f-e3045ac097e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''This code block contains functions that we need to use to analyze our flux sampling experiment. \n",
    "These include tests to check for convergence, for autocorrelation, as well as for pairwise comparisons of fluxes between\n",
    "parametization regimens (WT, TR, as well as 3 light conditions) and a script to determine whether fluxes are\n",
    "coupled with each other. Lastly, I also tried PCA to determine which reactions contribute most to the variance of each null space distribution per\n",
    "sampling run.\n",
    "'''\n",
    "\n",
    "\n",
    "def compare_pair(reaction, df1, df2, significance_threshold):\n",
    "    \"\"\"\n",
    "    Compare the flux distributions of a pair of columns from two dataframes using the Kruskal-Wallis test.\n",
    "\n",
    "    :param pair: Tuple containing the pair of column names.\n",
    "    :param df1: The first dataframe.\n",
    "    :param df2: The second dataframe.\n",
    "    :param significance_threshold: The significance threshold to use for the Kruskal-Wallis test.\n",
    "    :return: The pair and the result of the Kruskal-Wallis test.\n",
    "    \"\"\"\n",
    "    sample1 = df1[reaction]\n",
    "    sample2 = df2[reaction]\n",
    "    H, pval = kruskal(sample1, sample2)\n",
    "    return pval \n",
    "\n",
    "def compare_flux_distributions(df1, df2, significance_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Compare the flux distributions of each pair of columns from two dataframes using the Kruskal-Wallis test.\n",
    "\n",
    "    :param df1: The first dataframe.\n",
    "    :param df2: The second dataframe.\n",
    "    :param significance_threshold: The significance threshold to use for the Kruskal-Wallis test (default: 0.05).\n",
    "    :return: A list of pairs with significantly different distributions.\n",
    "    \"\"\"\n",
    "    common_columns = set(df1.columns) & set(df2.columns)\n",
    "    \n",
    "    pool = multiprocessing.Pool()\n",
    "    sig_results = []\n",
    "    non_sig_results = []\n",
    "    \n",
    "    for rxn in common_columns:\n",
    "        kw_pval = compare_pair(rxn, df1, df2, significance_threshold) #This does the Kruskal-Wallis part\n",
    "        if kw_pval < significance_threshold:\n",
    "            sig_results.append(rxn)\n",
    "        else:\n",
    "            non_sig_results.append(rxn)\n",
    "            \n",
    "    #Sort the outputs before returning\n",
    "    sig_results.sort()\n",
    "    non_sig_results.sort()\n",
    "            \n",
    "    \n",
    "    return sig_results, non_sig_results\n",
    "\n",
    "\n",
    "\n",
    "#The script above is used for flux coupling using the opened CSV file as input and outputs 3 lists:\n",
    "#positively correlated, neg. correlated and uncoupled reactions\n",
    "\n",
    "\n",
    "#This is for visualization\n",
    "def generate_histograms(dataframe1, dataframe2, column_name):\n",
    "    # Create subplots for two histograms\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "    # Histogram for dataframe1\n",
    "    axes[0].hist(dataframe1[column_name], bins=10, color='skyblue')\n",
    "    axes[0].set_title(f'Histogram of {column_name}')\n",
    "    axes[0].set_xlabel(column_name)\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    # Histogram for dataframe2\n",
    "    axes[1].hist(dataframe2[column_name], bins=10, color='lightgreen')\n",
    "    axes[1].set_title(f'Histogram of {column_name}')\n",
    "    axes[1].set_xlabel(column_name)\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the histograms\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def generate_stacked_histogram(dataframes, column_name,histtype='bar'):\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    # Concatenate the column data from all dataframes into a single series\n",
    "    \n",
    "    df_name = list()\n",
    "    \n",
    "    \n",
    "    for dfs in dataframes:\n",
    "    # Create the stacked histogram\n",
    "        if column_name in dfs.columns:\n",
    "\n",
    "            plt.hist(dfs[column_name], bins=33, alpha=0.45, stacked=True, density=False,histtype=histtype)\n",
    "            df_name.append(get_df_name(dfs))\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Set plot title and labels\n",
    "    plt.title(f\"Stacked Histogram for Column: {column_name}\")\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.legend(df_name)\n",
    " \n",
    "    \n",
    "    plt.figure().set_figheight(1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_scatter_regression(df, column_x, column_y):\n",
    "    \"\"\"\n",
    "    Generates a scatterplot and a linear regression line between two columns in a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        - df (pandas.DataFrame): The input dataframe.\n",
    "        - column_x (str): The column name for the X-axis.\n",
    "        - column_y (str): The column name for the Y-axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract X and Y values from the dataframe\n",
    "    X = df[column_x].values.reshape(-1, 1)\n",
    "    Y = df[column_y].values\n",
    "\n",
    "    # Fit linear regression model\n",
    "    regression = LinearRegression()\n",
    "    regression.fit(X, Y)\n",
    "\n",
    "    # Predict Y values based on the regression line\n",
    "    Y_pred = regression.predict(X)\n",
    "\n",
    "    # Plot the scatterplot and regression line\n",
    "    plt.scatter(X, Y, color='blue', label='Actual')\n",
    "    plt.plot(X, Y_pred, color='red', label='Regression Line')\n",
    "    plt.xlabel(column_x)\n",
    "    plt.ylabel(column_y)\n",
    "    plt.title('Scatterplot with Linear Regression Line')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#These functions are for generating PCA plots and other corollary analyses\n",
    "\n",
    "def merge_dataframes(dataframes, names):\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    for i, df in enumerate(dataframes):\n",
    "        # Get the corresponding name from the names list\n",
    "        df_name = names[i]\n",
    "        \n",
    "        # Add a new column with the dataframe name\n",
    "        df['sample'] = df_name\n",
    "        \n",
    "        # Merge dataframes\n",
    "        if merged_df.empty:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "    \n",
    "    # Remove NAs\n",
    "    merged_df = merged_df.fillna(1e-7)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def generate_pca_plot(dataframe):\n",
    "    numeric_columns = dataframe.select_dtypes(include=[np.number]).columns\n",
    "    X = dataframe[numeric_columns].values\n",
    "\n",
    "    pca = PCA(n_components=3)\n",
    "\n",
    "    components = pca.fit_transform(X)\n",
    "\n",
    "    # Extract dataframe names\n",
    "    dataframe_names = dataframe['sample'].unique()\n",
    "\n",
    "    # Generate plot with different colors for each dataframe\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'yellow']  # Add more colors if needed\n",
    "\n",
    "    #Generate figure and subplot\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    for i, name in enumerate(dataframe_names):\n",
    "        indices = dataframe['sample'] == name\n",
    "        ax.scatter(components[indices, 0], components[indices, 1], label=name, c=colors[i])\n",
    "\n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "    ax.set_zlabel('PC3')\n",
    "    ax.legend()\n",
    "    \n",
    "    mplcursors.cursor(hover=True)\n",
    "    plt.show()\n",
    " \n",
    "    #export PCA to determine component variance\n",
    "    \n",
    "    pca_fit = pca.fit(X)\n",
    "    \n",
    "    return pca_fit\n",
    "\n",
    "def select_significant_features(pca, feature_names, top_n):\n",
    "    loadings = pca.components_\n",
    "    abs_loadings = np.abs(loadings)\n",
    "    feature_contribution = np.sum(abs_loadings, axis=0)\n",
    "    sorted_features = feature_names[np.argsort(feature_contribution)[::-1]]\n",
    "    selected_features = sorted_features[:top_n]\n",
    "    return selected_features\n",
    "\n",
    "def plot_selected_features_heatmap(pca, feature_names, selected_features):\n",
    "    loadings = pca.components_\n",
    "    selected_indices = [np.where(feature_names == feature)[0][0] for feature in selected_features]\n",
    "    selected_loadings = loadings[:, selected_indices]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(selected_loadings, cmap='coolwarm', annot=False, xticklabels=selected_features)\n",
    "    plt.xlabel('Selected Features')\n",
    "    plt.ylabel('Principal Components')\n",
    "    plt.title('Loadings Heatmap of Selected Features')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "def visualize_correlation_matrix(df, sort_by=None, ascending=False):\n",
    "    \"\"\"\n",
    "    Calculates and visualizes the correlation matrix based on flux measurements in a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        - df (pandas.DataFrame): The input dataframe containing flux measurements.\n",
    "        - sort_by (str or None): Column name to sort the correlation matrix by. Default is None.\n",
    "        - ascending (bool): Whether to sort in ascending order. Default is False.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = df.corr()\n",
    "\n",
    "    # Sort the correlation matrix if sort_by is specified\n",
    "    if sort_by is not None:\n",
    "        correlation_matrix = correlation_matrix.sort_values(by=sort_by, ascending=ascending)\n",
    "\n",
    "    # Create a heatmap of the correlation matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True)\n",
    "\n",
    "    # Set plot title\n",
    "    plt.title('Correlation Matrix')\n",
    "\n",
    "    # Display the heatmap\n",
    "    plt.show()\n",
    "\n",
    "def filter_correlation_pairs(correlation_matrix, threshold, filter_elements):\n",
    "    \"\"\"\n",
    "    Filters out specific pairs in the correlation matrix based on a specified absolute value threshold,\n",
    "    removes pairs that share columns, removes symmetrical results, and returns the rows where the filter elements are found in either of the two variable columns.\n",
    "\n",
    "    Parameters:\n",
    "        - correlation_matrix (pandas.DataFrame): The correlation matrix.\n",
    "        - threshold (float): The absolute value threshold to filter the correlation coefficients.\n",
    "        - filter_elements (list): The elements to filter out from the correlation matrix.\n",
    "\n",
    "    Returns:\n",
    "        - filtered_pairs (pandas.DataFrame): The table showing the filtered pairs and their correlation coefficients,\n",
    "                                             where the filter elements are found in either of the two variable columns,\n",
    "                                             with symmetrical results removed.\n",
    "    \"\"\"\n",
    "    # Filter out pairs based on the threshold\n",
    "    filtered_pairs = correlation_matrix.unstack().reset_index()\n",
    "    filtered_pairs = filtered_pairs.rename(columns={'level_0': 'Variable A', 'level_1': 'Variable B', 0: 'Correlation'})\n",
    "    filtered_pairs = filtered_pairs[filtered_pairs['Correlation'].abs() >= threshold]\n",
    "\n",
    "    # Remove pairs that share columns\n",
    "    filtered_pairs = filtered_pairs[~(filtered_pairs['Variable A'] == filtered_pairs['Variable B'])]\n",
    "\n",
    "    # Remove symmetrical results\n",
    "    filtered_pairs = filtered_pairs[filtered_pairs['Variable A'] < filtered_pairs['Variable B']]\n",
    "\n",
    "    # Filter rows where the filter elements are found in either of the two variable columns\n",
    "    filtered_pairs = filtered_pairs[(filtered_pairs['Variable A'].isin(filter_elements)) | (filtered_pairs['Variable B'].isin(filter_elements))]\n",
    "    \n",
    "    # Sort the filtered pairs by correlation coefficient in descending order\n",
    "    filtered_pairs = filtered_pairs.sort_values('Correlation', ascending=False)\n",
    "    \n",
    "    filtered_pairs = filtered_pairs.reset_index()\n",
    "    \n",
    "\n",
    "    return filtered_pairs\n",
    "\n",
    "def check_column_exists(dataframes, column_name):\n",
    "    for df in dataframes:\n",
    "        if column_name in df.columns:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "###Functions for generating Reaction Charts\n",
    "\n",
    "\n",
    "def get_df_name(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    return name\n",
    "\n",
    "\n",
    "def load_csv_and_cleanup(filename, tol=1e-7):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Remove columns with a mean below tolerance\n",
    "    mean_values = df.mean()\n",
    "    columns_to_remove = mean_values[abs(mean_values) < tol].index\n",
    "    df = df.drop(columns=columns_to_remove)\n",
    "\n",
    "    # Fix the indices\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(df.shape)\n",
    "\n",
    "    return df    \n",
    "\n",
    "    \n",
    "\n",
    "def read_csv_to_dict(file_path):\n",
    "    data_dict = {}\n",
    "    with open(file_path, 'r', newline='') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        # Skip the header (first row)\n",
    "        next(csv_reader, None)\n",
    "        for row in csv_reader:\n",
    "            if len(row) == 3:\n",
    "                key = row[0]\n",
    "                value = (row[1], row[2])\n",
    "                if key in data_dict:\n",
    "                    data_dict[key].append(value)\n",
    "                else:\n",
    "                    data_dict[key] = [value]\n",
    "            else:\n",
    "                print(f\"Skipping row {row} as it does not contain exactly 3 columns.\")\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "\n",
    "def merge_dataframes(dataframes, names):\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    for i, df in enumerate(dataframes):\n",
    "        # Get the corresponding name from the names list\n",
    "        df_name = names[i]\n",
    "        \n",
    "        # Add a new column with the dataframe name\n",
    "        df['sample'] = df_name\n",
    "        \n",
    "        # Merge dataframes\n",
    "        if merged_df.empty:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "    \n",
    "    # Remove NAs\n",
    "    merged_df = merged_df.fillna(1e-7)\n",
    "      \n",
    "    merged_df[['model', 'ppfd']] = merged_df['sample'].str.split('_', 1, expand=True)\n",
    "    \n",
    "    \n",
    "    merged_df.drop(columns=['sample'], inplace=True)\n",
    "    \n",
    "    #Add the groups to the dataframe\n",
    "    return merged_df\n",
    "\n",
    "#Get model groups\n",
    "\n",
    "def preprocess_groups_list(dataframe, input_list, delimiter=';'):\n",
    "    \n",
    "    # Create a set to store the split elements temporarily (to avoid duplicates)\n",
    "    temp_set = set()\n",
    "\n",
    "    # Iterate through each element in the input_list\n",
    "    for element in input_list:\n",
    "        # Check if the delimiter is present in the element\n",
    "        if delimiter in element:\n",
    "            # Split the element using the delimiter and add the split elements to the temp_set\n",
    "            temp_set.update(element.split(delimiter))\n",
    "        else:\n",
    "            # If delimiter is not present, simply add the element to the temp_set\n",
    "            temp_set.add(element)\n",
    "\n",
    "    # Convert the temp_set back to a list before returning\n",
    "    result_list = list(temp_set)\n",
    "    return result_list\n",
    "\n",
    "\n",
    "def smallest_square_above(target):\n",
    "    # Calculate the square root of the target number\n",
    "    root = math.isqrt(target)\n",
    "    \n",
    "    # If the square of the root is less than or equal to the target, then increment the root by 1\n",
    "    if root * root <= target:\n",
    "        root += 1\n",
    "    \n",
    "    # Set the value of 'square' based on the value of 'root'\n",
    "    if root == 2:\n",
    "        square = 4\n",
    "    else:\n",
    "        square = root * root\n",
    "    \n",
    "    # Return the calculated square value\n",
    "    return square\n",
    "\n",
    "# def asterisk_significance(merged_df):\n",
    "\n",
    "def get_reaction_list_from_group(df,group, model):\n",
    "    '''this function gets a list of reactions \n",
    "    from the df columns based on whether they are part of a said subsystem (i.e. part of Calvin cycle)'''\n",
    "    \n",
    "    df_reactions = df.columns\n",
    "    \n",
    "    filt_reactions = list()\n",
    "    for rxns in model.reactions:\n",
    "        subsys = rxns.notes['SUBSYSTEM']\n",
    "        if group in subsys and rxns.id in df_reactions:\n",
    "            filt_reactions.append(rxns.id)\n",
    "            \n",
    "        \n",
    "    \n",
    "    return filt_reactions\n",
    "\n",
    "\n",
    "\n",
    "def generate_single_column_violin_plot(merged_df, column_name, filename=None, model=trans_model, dir='./plots/violin_plots/'):\n",
    "    # Extract data for the specified column_name\n",
    "    column_data = merged_df[column_name]\n",
    "    \n",
    "    # Compute the nearest square for the number of subplots (in this case, we only have one subplot)\n",
    "    dims = 1\n",
    "    \n",
    "    # Custom fig size based on gridspec dims\n",
    "    figsize = (5, 4)\n",
    "    sns.set(font_scale=0.7)\n",
    "    fig, axes = plt.subplots(nrows=dims, ncols=dims, figsize=figsize)\n",
    "    \n",
    "    # Generate violin plot for the specified column\n",
    "    sns.violinplot(data=merged_df, ax=axes, x='ppfd', y=column_name, hue='model')\n",
    "    \n",
    "    # Set plot title and axis labels\n",
    "    axes.set_title(f\"Violin Plot for '{column_name}'\")\n",
    "    axes.set_xlabel('Data')\n",
    "    axes.set_ylabel('Density')\n",
    "    \n",
    "    # Save the plot to the specified directory with the given filename\n",
    "    if filename:\n",
    "        plot_path = f\"{dir}{filename}.png\"\n",
    "        plt.savefig(plot_path)\n",
    "    \n",
    "    # Show the plot (optional, depends on your use case)\n",
    "    # plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def generate_violin_plot(merged_df, group,  return_out=False, sharey_axis=False, filename_suffix='', rxn_list='', model=trans_model, directory='./plots/violin_plots/'):\n",
    "    # Get list of reactions in merged_df\n",
    "    if rxn_list == '':\n",
    "        rxn_list = get_reaction_list_from_group(merged_df, group, model)\n",
    "    \n",
    "    \n",
    "    if rxn_list == 0:\n",
    "        return(f'No reactions in rxn group for {group}')\n",
    "    \n",
    "    \n",
    "    # Compute the nearest square for the number of subplots\n",
    "    dims = int(np.sqrt(smallest_square_above(len(rxn_list))))\n",
    "    \n",
    "    # Custom fig size based on gridspec dims\n",
    "    figsize = (4 * dims, 3 * dims)\n",
    "    sns.set(font_scale=1)\n",
    "    fig, axes = plt.subplots(nrows=dims, ncols=dims, figsize=figsize, squeeze=False, sharey=sharey_axis)  \n",
    "    fig.text(0.5, 0.04, 'Photon Flux (umol/m2/s)', ha='center')  # Adding the shared x-axis label\n",
    "    fig.text(0.0006, 0.5, 'Flux values (umol/m^2s)', va='center', rotation='vertical')  # Adding the shared y-axis label\n",
    "   \n",
    "    zip_values = itertools.zip_longest(axes.flat, rxn_list, fillvalue=None)\n",
    "    ppfd_vals = ['250', '750', '1500']\n",
    "    \n",
    "    for ax, rxn in zip_values:\n",
    "        if rxn is not None:\n",
    "            g=sns.violinplot(data=merged_df, x='ppfd', y=rxn, hue='model', bw='scott', order=ppfd_vals, ax=ax)\n",
    "            g.set(ylabel='')\n",
    "            g.set(xlabel='')\n",
    "            g.set(title=rxn)\n",
    "            # ax.set_ylabel('Flux samples (umol/m2s)')\n",
    "\n",
    "            # Removing individual x-axis labels for subplots\n",
    "            \n",
    "            ax.xaxis.labelpad = 10\n",
    "            \n",
    "            # Removing the legend for individual subplots (assuming the legend will be placed outside the subplots)\n",
    "            ax.get_legend().remove()\n",
    "        else:\n",
    "            ax.margins(x=0, y=0)\n",
    "            ax.set_axis_off()\n",
    "    \n",
    "    # Add a title for the whole plot\n",
    "    fig.suptitle(f\"Reactions in the '{group}' group\")\n",
    "    fig.subplots_adjust(top=0.88)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    #Remove any special characters from the groups so that it has no issues  with filenames\n",
    "    group = re.sub(r'[\\\\/:\"*?<>|]+', '_', group)\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "    filename = f'{group}_{filename_suffix}.png'\n",
    "    plot_path = os.path.join(directory, filename)\n",
    "    plt.savefig(plot_path)\n",
    "    \n",
    "    if return_out:\n",
    "        return fig, axes\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "        \n",
    "        \n",
    "#Generate box plot\n",
    "\n",
    "def generate_box_plot(merged_df, group, return_out=False, filename_suffix='', rxn_list='',model=trans_model, directory='./plots/violin_plots/'):\n",
    "    # Get list of reactions in merged_df\n",
    "    if rxn_list == '':\n",
    "        rxn_list = get_reaction_list_from_group(merged_df, group, model)\n",
    "    \n",
    "        \n",
    "    if rxn_list == 0:\n",
    "        return(f'No reactions in rxn group for {group}')\n",
    "    \n",
    "    #Ensure that the reaction is in the columns of the df\n",
    "    for rxns in rxn_list:\n",
    "        if rxns not in merged_df.columns:\n",
    "            rxn_list = rxn_list.remove(rxns)\n",
    "    \n",
    "    # Compute the nearest square for the number of subplots\n",
    "    dims = int(np.sqrt(smallest_square_above(len(rxn_list))))\n",
    "    \n",
    "    # Custom fig size based on gridspec dims\n",
    "    figsize = (5 * dims, 3 * dims)\n",
    "    sns.set(font_scale=1)\n",
    "    fig, axes = plt.subplots(nrows=dims, ncols=dims, figsize=figsize, squeeze=False)  \n",
    "    fig.text(0.5, 0.004, 'Photon Flux (umol/m2/s)', ha='center')  # Adding the shared x-axis label\n",
    "    fig.text(0.0006, 0.5, 'Flux values (umol/m^2s)', va='center', rotation='vertical')  # Adding the shared y-axis label\n",
    "   \n",
    "    zip_values = itertools.zip_longest(axes.flat, rxn_list, fillvalue=None)\n",
    "    ppfd_vals = ['250', '750', '1500']\n",
    "    \n",
    "    for ax, rxn in zip_values:\n",
    "        if rxn is not None:\n",
    "            g=sns.boxplot(data=merged_df, x='ppfd', y=rxn, hue='model', order=ppfd_vals, ax=ax, showfliers=False)\n",
    "            g.axhline(0, linestyle='-', linewidth=0.05) #Adds a horizontal line \n",
    "            ax.set_title(rxn)\n",
    "            ax.set_xlabel('')\n",
    "            # ax.set_ylabel('Flux samples (umol/m2s)')\n",
    "\n",
    "            # Removing individual x-axis labels for subplots\n",
    "            \n",
    "            ax.xaxis.labelpad = 10\n",
    "            \n",
    "            # Removing the legend for individual subplots (assuming the legend will be placed outside the subplots)\n",
    "            ax.get_legend().remove()\n",
    "        else:\n",
    "            ax.margins(x=0, y=0)\n",
    "            ax.set_axis_off()\n",
    "            fig.delaxes(ax) \n",
    "    \n",
    "    # Add a title for the whole plot\n",
    "    fig.suptitle(f\"Reactions in the '{group}' group\")\n",
    "    fig.subplots_adjust(top=0.88)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #Remove any special characters from the groups so that it has no issues  with filenames\n",
    "    group = re.sub(r'[\\\\/:\"*?<>|]+', '_', group)\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "    filename = f'{group}_{filename_suffix}.png'\n",
    "    plot_path = os.path.join(directory, filename)\n",
    "    plt.savefig(plot_path)\n",
    "    \n",
    "    if return_out:\n",
    "        return fig, axes\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "        \n",
    "        \n",
    "\n",
    "def drop_cols(df, string):\n",
    "    columns_to_drop = [col for col in df.columns if string in col]\n",
    "    return df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "505994c4-1e2d-442d-b90e-e269414aecd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10008, 697)\n",
      "(10008, 814)\n",
      "(10008, 750)\n",
      "(10008, 1036)\n",
      "(10008, 1119)\n",
      "(10008, 1185)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5916/3608730344.py:364: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
      "  merged_df[['model', 'ppfd']] = merged_df['sample'].str.split('_', 1, expand=True)\n"
     ]
    }
   ],
   "source": [
    "#Test the script on one CSV\n",
    "#Load only reactions that have a flux higher than 0.001 umol/m^2s? I think that is a reasonable cutoff\n",
    "\n",
    "wt_250 = load_csv_and_cleanup('./flux_results/flux_sampling/8th_attempt_fixed_transporter_issues/10k_samples/WT_250.csv', tol = 1e-4)\n",
    "wt_750 = load_csv_and_cleanup('./flux_results/flux_sampling/8th_attempt_fixed_transporter_issues/10k_samples/WT_750.csv', tol = 1e-4)\n",
    "wt_1500 = load_csv_and_cleanup('./flux_results/flux_sampling/8th_attempt_fixed_transporter_issues/10k_samples/WT_1500.csv', tol = 1e-4)\n",
    "tr_250 = load_csv_and_cleanup('./flux_results/flux_sampling/8th_attempt_fixed_transporter_issues/10k_samples/TR_250.csv', tol = 1e-4)\n",
    "tr_750 = load_csv_and_cleanup('./flux_results/flux_sampling/8th_attempt_fixed_transporter_issues/10k_samples/TR_750.csv', tol = 1e-4)\n",
    "tr_1500 = load_csv_and_cleanup('./flux_results/flux_sampling/8th_attempt_fixed_transporter_issues/10k_samples/TR_1500.csv', tol=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "df_list = [wt_250, wt_750, wt_1500, tr_250, tr_750, tr_1500]\n",
    "names = ['WT_250', 'WT_750', 'WT_1500', 'TR_250', 'TR_750', 'TR_1500']\n",
    "wt_list = [wt_250, wt_750, wt_1500]\n",
    "tr_list = [tr_250, tr_750, tr_1500]\n",
    "_250_list = [wt_250,tr_250]\n",
    "_750_list = [wt_750,tr_750]\n",
    "_1500_list=[wt_1500,tr_1500]\n",
    "\n",
    "#For testing outputs\n",
    "\n",
    "%matplotlib inline\n",
    "merged_df = merge_dataframes(df_list, names)\n",
    "test = generate_violin_plot(merged_df, 'Calvin cycle', filename_suffix='test_plot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b5d1fcf-ce50-47b8-a652-f94f81ccd260",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5916/3608730344.py:364: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
      "  merged_df[['model', 'ppfd']] = merged_df['sample'].str.split('_', 1, expand=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing plots!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5916/3608730344.py:364: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
      "  merged_df[['model', 'ppfd']] = merged_df['sample'].str.split('_', 1, expand=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    #Merge dataframes\n",
    "    merged_df = merge_dataframes(df_list, names)\n",
    "    #Get group ids\n",
    "    groups_ids = [i.id for i in wt_model.groups]\n",
    "    #Pre-process groups list \n",
    "    groups_list = preprocess_groups_list(merged_df,groups_ids)\n",
    "    \n",
    "    #Filter dfs to get only M and BS cell reactions each\n",
    "    m_dataset = drop_cols(merged_df, \"_BS\")\n",
    "    bs_dataset = drop_cols(merged_df, \"_M\")\n",
    "    \n",
    "    for group in groups_list:\n",
    "        \n",
    "        \n",
    "        #Check if reaction is non zero else go to next\n",
    "        while True:\n",
    "            if len(get_reaction_list_from_group(m_dataset, group, trans_model)) != 0:\n",
    "                # print(f'reaches M here {group}')\n",
    "                # generate_violin_plot(m_dataset, group, filename_suffix=\"M\", directory='./plots/8th-Attempt-0810-2023-final/violin_plots/M_cell')\n",
    "                generate_box_plot(m_dataset, group, filename_suffix=\"M\", directory='./plots/8th-Attempt-0810-2023-final/box_plots/M_cell')\n",
    "                break\n",
    "            else:\n",
    "                break\n",
    "        while True:\n",
    "            if len(get_reaction_list_from_group(bs_dataset, group, trans_model)) != 0:\n",
    "                # print(f'reaches BS here {group}')\n",
    "                # generate_violin_plot(bs_dataset, group, filename_suffix=\"BS\", directory='./plots/8th-Attempt-0810-2023-final/violin_plots/BS_cell')\n",
    "                generate_box_plot(bs_dataset, group, filename_suffix=\"BS\", directory='./plots/8th-Attempt-0810-2023-final/box_plots/BS_cell')\n",
    "                break\n",
    "            else:\n",
    "                break #Break out of True condition\n",
    "        \n",
    "    print('Finished writing plots!')\n",
    "        \n",
    "main()\n",
    "\n",
    "\n",
    "merged_df = merge_dataframes(df_list, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "429e5572-87f5-409c-8d3d-f9b85605aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial \n",
    "\n",
    "def filter_rxns_group(df, rxn_list):\n",
    "    filtered_list = list()\n",
    "    for rxns in rxn_list:\n",
    "        if rxns in df.columns:\n",
    "            filtered_list.append(rxns)\n",
    "    return filtered_list\n",
    "\n",
    "def get_aggregate_reactions_mets(model,df, mets_to_search=''):\n",
    "    #This codeblock gets all reactions based on a metabolite string\n",
    "\n",
    "    met_list = list()\n",
    "    \n",
    "    for mets in model.metabolites:\n",
    "        for i in mets_to_search:\n",
    "            if i in mets.id:\n",
    "                met_list.append(mets)\n",
    "\n",
    "    rxn_list=list()\n",
    "    #This codeblock gets all the reactions that have the metabolite in its reaction\n",
    "    for mets in met_list:\n",
    "        for rxns in model.reactions:\n",
    "            if mets in rxns.metabolites:\n",
    "                rxn_list.append(rxns.id)\n",
    "                \n",
    "    filtered_list =  list()\n",
    "    for rxns in rxn_list:\n",
    "        if rxns in df.columns:\n",
    "            filtered_list.append(rxns)\n",
    "    return filtered_list\n",
    "                \n",
    "\n",
    "\n",
    "def get_aggregate_reactions_rxns(model,df, rxns_to_search=''):\n",
    "    #This codeblock gets all reactions in all compartments based on a given keyword \n",
    "\n",
    "    \n",
    "    rxn_list_main=list()\n",
    "                 \n",
    "    for i in rxns_to_search:\n",
    "        rxns_list= [rxns.id for rxns in model.reactions if i in rxns.id]\n",
    "        rxn_list_main.extend(rxns_list)\n",
    "                \n",
    "    filtered_list =  list()\n",
    "    for rxns in rxn_list_main:\n",
    "        if rxns in df.columns:\n",
    "            filtered_list.append(rxns)\n",
    "    return filtered_list\n",
    "                \n",
    "    \n",
    "    \n",
    "    \n",
    "custom_dir = './plots/8th-Attempt-0810-2023-final/box_plots/custom_groups/'\n",
    "    \n",
    "nad_s0 = trans_model.metabolites.nad_s0\n",
    "    \n",
    "# #Add custom groups \n",
    "# df_cols = merged_df.columns\n",
    "\n",
    "\n",
    "co2_transport = get_aggregate_reactions_rxns(trans_model, merged_df, ['CO2t', 'co2_pd', 'HCO3'])\n",
    "pd_transport = get_aggregate_reactions_rxns(trans_model, merged_df, ['_pd'])\n",
    "nad_reactions = get_aggregate_reactions_mets(trans_model, merged_df,['nad_s'])\n",
    "nadp_reactions = get_aggregate_reactions_mets(trans_model, merged_df,['nadp_s'])\n",
    "mal_reactions = get_aggregate_reactions_mets(trans_model, merged_df, ['mal-L_s0', 'mal-L_m0', 'mal-L_c0', 'mal-L_x0'])\n",
    "dhap_reactions = get_aggregate_reactions_mets(trans_model, merged_df,['dhap'])\n",
    "cit_rxns = get_aggregate_reactions_mets(trans_model, merged_df,['cit'])\n",
    "\n",
    "\n",
    "gvp = partial(generate_box_plot, merged_df=merged_df, directory=custom_dir)\n",
    "\n",
    "gvp(group='CO2 transporters', rxn_list=co2_transport)\n",
    "            \n",
    "gvp(group='pd reactions', rxn_list=pd_transport)\n",
    "\n",
    "gvp(group='NAD reactions (plastidial)', rxn_list=nad_reactions)\n",
    "gvp(group='NADP reactions (plastidial)', rxn_list=nadp_reactions)\n",
    "\n",
    "gvp(group='Malate-related reactions (M Cell)', rxn_list=mal_reactions)\n",
    "            \n",
    "    \n",
    "gvp(group='DHAP related reactions', rxn_list=dhap_reactions)\n",
    "gvp(group='Citrate related reactions', rxn_list=cit_rxns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a86cea6-7558-4b9f-9c70-65719c71cf05",
   "metadata": {},
   "source": [
    "#Algorithm for generating plots based  on groups//\n",
    "\n",
    "\n",
    "#Combine all the datasets adding the category 'WT and TR' as a column 'model' //\n",
    "\n",
    "#Get the list of groups per reaction //\n",
    "#Obtain the size of the gridspec needed by finding nearest square to the list //\n",
    "\n",
    "#Obtain the flux ranges to add error bars to the violin plot signifying the low and high ranges using FVA//\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Iterate over the list, generate violin plots showing WT and TR models across the 3 light conditions //\n",
    "#Add hue with 'model' as column from dataframe and 'light_condition' as x and the predicted flux values as y//\n",
    "\n",
    "Notes:\n",
    "\n",
    "Basic plot works now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f059f6b9-31d3-4108-aa3c-d9ecca893b76",
   "metadata": {},
   "source": [
    "#Algorithm for generating plots based  on groups//\n",
    "\n",
    "\n",
    "#Get the list of groups per reaction //\n",
    "#Obtain the size of the gridspec needed by finding nearest square to the list\n",
    "\n",
    "#Obtain the flux ranges to add error bars to the violin plot signifying the low and high ranges using FVA//\n",
    "\n",
    "#Combine all the datasets adding the category 'WT and TR' as a column 'model'//\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Iterate over the list, generate violin plots showing WT and TR models across the 3 light conditions \n",
    "    Almost done but for some reason it doesn't show\n",
    "#Add hue with 'model' as column from dataframe and 'light_condition' as x and the predicted flux values as y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592914ef-5101-42da-953b-211da4cc1728",
   "metadata": {},
   "source": [
    "Findings for the heat plot\n",
    "\n",
    "I don't think the heatplot of the log2 fold changes is too informative considering that the flux ranges are the actual meat of the analysis.\n",
    "\n",
    "What I should do is to prepare plots for all groups instead?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c25140-27e5-43eb-92f5-5af51ad9e06b",
   "metadata": {},
   "source": [
    "Inferences based on filtered heatmap:\n",
    "\n",
    "- Significantly differential fluxes in the Glycolysis module in the Bundle sheath compartment. Is it due to the supply of Triose Phosphates from the Mesophyll Cell?\n",
    "- No significant perturbations seen in M Cell Calvin Cycle module.\n",
    "- RPSII flux seen to be higher in TR rice than in WT -- indicative of change in CEF/LEF flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c306e-5065-4d39-96cd-938a4f6ec639",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Hmm. Interesting results. It shows that fluxes have become more decoupled in the low light regimens compared with medium and high light regimens when comparing results between parametrizations (WT and TR), at a threshold value of 0.001.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
